<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>spark学习杂记</title>
      <link href="/2026/01/22/spark%E5%AD%A6%E4%B9%A0%E6%9D%82%E8%AE%B0/"/>
      <url>/2026/01/22/spark%E5%AD%A6%E4%B9%A0%E6%9D%82%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h2 id="Spark的核心概念"><a href="#Spark的核心概念" class="headerlink" title="Spark的核心概念"></a>Spark的核心概念</h2><h3 id="1-RDD–-Spark的核心基石"><a href="#1-RDD–-Spark的核心基石" class="headerlink" title="1.RDD–&gt;Spark的核心基石"></a>1.RDD–&gt;Spark的核心基石</h3><p><strong>RDD</strong> 的全称是 <strong>Resilient Distributed Dataset</strong>，中文是“<strong>弹性分布式数据集</strong>”。我们把它拆开来看：</p><ul><li><strong>Dataset (数据集)</strong>：它就是一个集合，里面存放着你的数据。</li><li><strong>Distributed (分布式)</strong>：这个数据集不是存放在一台机器上，而是被切分成很多小块（称为“分区”），分散存储在集群的多台机器上。这样做的好处是，可以利用多台机器的计算能力并行处理这些数据，大大提升了速度。</li><li><strong>Resilient (弹性的&#x2F;可容错的)</strong>：这是 RDD 最核心的特性。如果存储某个数据分区的机器宕机了，数据丢失了怎么办？没关系，Spark 会根据数据的“血缘关系”（Lineage）——也就是记录了这个数据分区是通过哪些计算步骤得来的——自动在另一台正常的机器上重新计算出丢失的数据。因此，它具有很强的容错能力，像一个不倒翁。</li></ul><p>简单总结：<strong>RDD 是一个被分区、不可变、支持并行操作的分布式数据集合，并且拥有强大的自动容错能力。</strong></p><p>RDD的特性如下：</p><ol><li>不可变性：RDD 一旦被创建就不能被修改。如果想“修改”它，你只能通过一个“转换操作 (Transformation)”来生成一个<strong>新的 RDD</strong>。</li><li>惰性：当你对一个 RDD 执行转换操作时（比如 <code>map</code>, <code>filter</code>），Spark 并<strong>不会立即计算</strong>。它只是默默地记下这个操作，把它加入到“血缘关系”的图（DAG）中。只有当一个“行动操作 (Action)”被触发时（比如 <code>count</code>, <code>collect</code>, <code>save</code>），Spark 才会真正开始执行从头到尾的所有计算。</li><li>血缘（DAG）：也称为 DAG (Directed Acyclic Graph, 有向无环图)。这是 RDD 容错机制的核心，就像“乐高说明书”。它记录了数据从源头到最终结果的每一步转换。</li></ol><h3 id="2-Spark的主流API-dataFrame"><a href="#2-Spark的主流API-dataFrame" class="headerlink" title="2.Spark的主流API-dataFrame"></a>2.Spark的主流API-dataFrame</h3><p><strong>DataFrame</strong>，中文是“数据帧”。它本质上也是一个分布式的数据集合，但它更像我们非常熟悉的<strong>关系型数据库中的表 (Table)</strong> 或者一个 <strong>Excel 电子表格</strong>。</p><p>与 RDD 不同的是，DataFrame 的数据是<strong>结构化的</strong>，它有明确的<strong>列名 (Columns)</strong> 和每一列的数据<strong>类型 (Schema)</strong>。</p><h3 id="3-DataSet-DataFrame的扩展"><a href="#3-DataSet-DataFrame的扩展" class="headerlink" title="3.DataSet -&gt; DataFrame的扩展"></a>3.DataSet -&gt; DataFrame的扩展</h3><p><strong>DataSet</strong> 是在 Spark 1.6 中引入的，可以看作是 DataFrame 的一个<strong>扩展</strong>。它试图结合 DataFrame 的性能优势和 RDD 的类型安全特性。</p><ul><li><p><strong>在 Java&#x2F;Scala 中</strong>：DataSet 是一个<strong>强类型</strong>的 API。这意味着你在编译时就能检查出类型错误。例如，你定义一个 <code>DataSet[Person]</code>，那么集合里的每个元素都必须是 <code>Person</code> 对象，如果你试图往里面放一个 <code>Car</code> 对象，编译器会直接报错。这让代码更健壮。</p></li><li><p><strong>在 Python (PySpark) 中</strong>：这是一个<strong>非常非常重要</strong>的点！<strong>Python 语言本身是动态类型的，所以 PySpark 中没有一个独立的、像 Java&#x2F;Scala 中那样强类型的 DataSet API。在 PySpark 里，DataFrame 就是 DataSet 的一个特例，可以认为 <code>DataFrame = DataSet[Row]</code></strong>。Row 是一个通用的行对象。</p></li></ul><p><em><strong>对于Python而言</strong></em>：DataSet就是DataFrame</p><h2 id="如何理解Spark相比于MapReduce的优势？"><a href="#如何理解Spark相比于MapReduce的优势？" class="headerlink" title="如何理解Spark相比于MapReduce的优势？"></a>如何理解Spark相比于MapReduce的优势？</h2><h3 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h3><p><strong>MapReduce</strong> 的核心思想其实就是八个字：<strong>分而治之，先分后总</strong><br>假设需要统计一个图书馆的每个单词各自出现了多少次。一个人肯定干不完，就会采用MapReduce的思想：</p><ol><li><p><strong>Map (映射&#x2F;分工)</strong>: 您把图书馆的书分成很多份，找来一大群图书管理员（比如100个），每人分几架书。 您给他们下达一个<strong>完全相同</strong>的简单指令：“你们每个人，把自己负责的书从头到尾读一遍，每遇到一个单词，就在一张小卡片上写下 <code>(这个单词, 1)</code>。比如读到’Hello’，就写一张 <code>(Hello, 1)</code> 的卡片。” 这个过程就是 <strong>Map</strong>。它的本质是把一个大任务，<strong>切分</strong>成无数个可以<strong>并行处理</strong>的小任务，并把原始数据处理成一种统一的格式（这里就是 <code>(单词, 1)</code> 这种键值对）。</p></li><li><p><strong>Reduce (归约&#x2F;汇总)</strong>: 现在，您面前堆满了山一样的小卡片。您又找来另一批管理员。 您让他们先把所有卡片按照单词的首字母排序、整理。把所有写着 <code>(Hello, 1)</code> 的卡片放在一起，所有 <code>(World, 1)</code> 的卡片放在一起，以此类推。 然后，您给每个单词指派一位“汇总员”。负责’Hello’的汇总员，只需要数一下他面前有多少张 <code>(Hello, 1)</code> 的卡片，比如有5000张，他最后就向您汇报一个结果：<code>(Hello, 5000)</code>。 这个过程就是 <strong>Reduce</strong>。它的本质是对Map阶段处理好的、已经按“键”（Key，这里指单词）分类的数据进行<strong>汇总</strong>和<strong>计算</strong>，得出最终结果。</p></li></ol><p>MapReduce的中间结果，也就是上面例子的小卡片，是需要存入硬盘的。假设上面例子的图书馆，它的地面是<strong>硬盘（HDFS）</strong>。</p><ul><li>当Map阶段的图书管理员们写好 <code>(单词, 1)</code> 的小卡片后，他们必须把这些卡片<strong>扔在地上（写入硬盘）</strong>。</li><li>然后，系统要等所有人都扔完，再去地上把所有卡片捡起来、分类、排序。</li><li>最后，Reduce阶段的汇总员们，再从地上<strong>捡起</strong>属于自己的那一堆卡片（比如所有’Hello’的卡片），进行汇总。</li></ul><h3 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h3><p>Spark是基于内存计算的，回到上面的例子，这个“扔在地上”再“捡起来”的动作，也就是**“写入硬盘”<strong>和</strong>“读取硬盘”**，是非常耗费时间的。</p><p>在Spark的世界里，图书馆里配备了许多张巨大的<strong>工作台（内存）</strong>。</p><ul><li>Map阶段的管理员们写好 <code>(单词, 1)</code> 的卡片后，不再扔到地上，而是整齐地摆在自己的工作台上。</li><li>当需要进行下一步（比如Reduce）时，汇总员们可以直接从这些工作台上拿走卡片进行处理，速度飞快。</li></ul><p>所以，Spark相比于MapReduce最核心的优势就是：<strong>它尽可能地将计算过程中的中间数据存放在速度极快的内存中，而不是每次都写入缓慢的硬盘，从而极大地提升了计算效率。</strong></p><p><em><strong>简而言之，Spark 通过内存计算，避开了MapReduce昂贵的磁盘I&#x2F;O开销</strong></em></p><h3 id="如何使用-PySpark-DataFrame-API-从-HDFS-读取文件并进行操作"><a href="#如何使用-PySpark-DataFrame-API-从-HDFS-读取文件并进行操作" class="headerlink" title="如何使用 PySpark DataFrame API 从 HDFS 读取文件并进行操作"></a>如何使用 PySpark DataFrame API 从 HDFS 读取文件并进行操作</h3><p>主要分3步：</p><ol><li>建立连接（<strong><code>SparkSession</code></strong>）：您、需要创建一个<code>SparkSession</code>对象，它是程序和Spark集群沟通的唯一入口。</li><li><strong>下达读取指令 (<code>spark.read</code>)</strong>: 有了指挥部，您就可以下达指令了。<code>spark.read</code> 就是“读取”这个动作的开始。</li><li><strong>指定目标和格式 (<code>.format().load()</code>)</strong>: 您需要告诉士兵们：<ul><li>要读取的文件是什么<strong>格式</strong>？（例如是CSV, JSON, 还是Parquet）—— 这通过 <code>.format(&quot;csv&quot;)</code> 来指定。</li><li>文件具体放在仓库的<strong>哪个位置</strong>？—— 这通过 <code>.load(&quot;hdfs://...&quot;)</code> 来指定。HDFS的路径通常以 <code>hdfs://</code> 开头。<br>下面是描述这个过程的伪代码：</li></ul></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. 导入并创建 SparkSession (你的“指挥部”)</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">&quot;ReadHDFSExample&quot;</span>).getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 定义 HDFS 上的文件路径</span></span><br><span class="line"><span class="comment"># 这里的 &lt;namenode_host&gt;:&lt;port&gt; 是你HDFS主节点的地址和端口</span></span><br><span class="line">hdfs_path = <span class="string">&quot;hdfs://&lt;namenode_host&gt;:&lt;port&gt;/path/to/your/file.csv&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 使用 DataFrame API 读取文件</span></span><br><span class="line"><span class="comment"># .format(&quot;csv&quot;) -&gt; 指定文件格式是CSV</span></span><br><span class="line"><span class="comment"># .option(&quot;header&quot;, &quot;true&quot;) -&gt; 告诉Spark第一行是表头，而不是数据</span></span><br><span class="line"><span class="comment"># .load(hdfs_path) -&gt; 从指定HDFS路径加载数据</span></span><br><span class="line">df = spark.read.<span class="built_in">format</span>(<span class="string">&quot;csv&quot;</span>) \</span><br><span class="line">            .option(<span class="string">&quot;header&quot;</span>, <span class="string">&quot;true&quot;</span>) \</span><br><span class="line">            .load(hdfs_path)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 对创建的 DataFrame 进行一个简单操作，比如打印它的结构</span></span><br><span class="line">df.printSchema()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. 显示前几行数据，确认读取成功</span></span><br><span class="line">df.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6. 任务完成，关闭指挥部，释放资源</span></span><br><span class="line">spark.stop()</span><br></pre></td></tr></table></figure><h3 id="SparkSQL"><a href="#SparkSQL" class="headerlink" title="SparkSQL"></a>SparkSQL</h3><p><strong>Spark SQL</strong> 是Apache Spark中的一个核心模块，专门用于处理<strong>结构化数据</strong>。<br>它有两个主要特点：</p><ol><li><strong>统一入口</strong>：无论您的数据存储在何处（例如，JSON文件、CSV文件、数据库表，或者Hive表），您都可以使用标准的SQL查询语句来对它们进行查询和分析。</li><li><strong>高性能</strong>：Spark SQL底层利用了Spark核心引擎的先进技术（如内存计算和强大的Catalyst优化器），使得SQL查询的执行速度非常快，比传统的Hive on MapReduce要快得多。</li></ol><p>在使用SQL前，要先准备环境和数据，示例代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个SparkSession实例</span></span><br><span class="line">spark = SparkSession.builder \</span><br><span class="line">    .appName(<span class="string">&quot;SparkSQLExample&quot;</span>) \</span><br><span class="line">    .getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为了方便演示，我们不从文件读取，而是直接在内存中创建数据</span></span><br><span class="line"><span class="comment"># 这等同于我们有一个名为 &quot;scores.csv&quot; 的文件，内容如下：</span></span><br><span class="line"><span class="comment"># name,subject,score</span></span><br><span class="line"><span class="comment"># Alice,Math,95</span></span><br><span class="line"><span class="comment"># Bob,Math,88</span></span><br><span class="line"><span class="comment"># Alice,English,89</span></span><br><span class="line"><span class="comment"># Charlie,Math,92</span></span><br><span class="line"><span class="comment"># Bob,English,94</span></span><br><span class="line">data = [(<span class="string">&quot;Alice&quot;</span>, <span class="string">&quot;Math&quot;</span>, <span class="number">95</span>),</span><br><span class="line">        (<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;Math&quot;</span>, <span class="number">88</span>),</span><br><span class="line">        (<span class="string">&quot;Alice&quot;</span>, <span class="string">&quot;English&quot;</span>, <span class="number">89</span>),</span><br><span class="line">        (<span class="string">&quot;Charlie&quot;</span>, <span class="string">&quot;Math&quot;</span>, <span class="number">92</span>),</span><br><span class="line">        (<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;English&quot;</span>, <span class="number">94</span>)]</span><br><span class="line">columns = [<span class="string">&quot;name&quot;</span>, <span class="string">&quot;subject&quot;</span>, <span class="string">&quot;score&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据转换成Spark的结构化数据格式：DataFrame</span></span><br><span class="line">df = spark.createDataFrame(data, columns)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;成功创建DataFrame，数据如下：&quot;</span>)</span><br><span class="line">df.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>现在虽然我们有了一个df，但是想要用上Spark SQL,我们还得为他注册一个表名，示例代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将DataFrame注册为一个临时的SQL视图（表）</span></span><br><span class="line"></span><br><span class="line">df.createOrReplaceTempView(<span class="string">&quot;scores_table&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;DataFrame已成功注册为名为 &#x27;scores_table&#x27; 的临时表。&quot;</span>)</span><br></pre></td></tr></table></figure><blockquote><p><em><strong>createOrReplaceTempView</strong></em>: 这是连接DataFrame世界和SQL世界的<strong>桥梁</strong>。它创建了一个只在当前 <code>SparkSession</code> 中有效的临时表。这个表是“虚拟”的，它并不真正复制数据，只是给 <code>df</code> 起了一个SQL可以识别的别名。会话结束后，这个临时表会自动消失。</p></blockquote><p>现在我们可以用SQL查询数据了，示例SQL：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用 spark.sql() 来执行标准的SQL查询</span></span><br><span class="line">high_scores_df = spark.sql(<span class="string">&quot;SELECT name, score FROM scores_table WHERE score &gt; 90&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;查询所有分数高于90分的学生：&quot;</span>)</span><br><span class="line">high_scores_df.show()</span><br></pre></td></tr></table></figure><blockquote><p><em><strong>spark.sql()</strong></em>: 这是Spark SQL的执行入口。您可以在括号里的字符串中编写任何标准的SQL语句。它的返回值是<strong>另一个DataFrame</strong>。这是Spark非常优雅的一点：无论您使用DataFrame API还是SQL，输入和输出的类型都是统一的。</p></blockquote><h4 id="Hive-和-Spark-SQL的异同"><a href="#Hive-和-Spark-SQL的异同" class="headerlink" title="Hive 和 Spark SQL的异同"></a>Hive 和 Spark SQL的异同</h4><ol><li>Hive是传统的MapReduce，速度会比Spark慢；</li><li>Hive是永久的，Spark加载到内存里面，数据等，会消失；因此Hive适合作为数据仓库，Spark适合快速探索数据</li><li>Spark SQL可以创建<code>TempView</code>（临时视图），非常灵活；Hive的表是永久记录在<strong>Metastore</strong>（元数据中心）中的，更持久、更严格。</li></ol><p>**注：Spark SQL非常强大，它也可以连接到Hive的Metastore，把Hive的永久表当作自己的数据源来查询。这时候，Spark SQL就扮演了一个“更快的叉车”角色，去Hive的“仓储超市”里拉货，这被称为 <strong>“Spark on Hive”</strong>。</p><h3 id="对比表格"><a href="#对比表格" class="headerlink" title="对比表格"></a><strong>对比表格</strong></h3><table><thead><tr><th align="left">特性</th><th align="left">Hive (传统中央厨房)</th><th align="left">Spark SQL (现代智能厨房)</th></tr></thead><tbody><tr><td align="left"><strong>执行引擎</strong></td><td align="left"><strong>MapReduce</strong> (基于磁盘，分步，慢)</td><td align="left"><strong>Spark Core</strong> (基于内存, 流水线, 快)</td></tr><tr><td align="left"><strong>核心数据模型</strong></td><td align="left">文件 (HDFS) + 元数据 (Metastore)</td><td align="left"><strong>DataFrame&#x2F;Dataset</strong> (分布式内存对象)</td></tr><tr><td align="left"><strong>查询优化</strong></td><td align="left">基础的优化</td><td align="left"><strong>Catalyst优化器</strong> (非常智能，会自动改写和优化查询计划)</td></tr><tr><td align="left"><strong>数据源</strong></td><td align="left">主要依赖HDFS和Hive表</td><td align="left"><strong>多样化</strong> (HDFS, S3, JSON, CSV, JDBC, Hive表等)</td></tr><tr><td align="left"><strong>交互性</strong></td><td align="left"><strong>低</strong> (查询通常需要几分钟到几小时)</td><td align="left"><strong>高</strong> (查询通常在几秒到几分钟内完成)</td></tr><tr><td align="left"><strong>应用场景</strong></td><td align="left">数据仓库、大规模ETL、批处理报表</td><td align="left">交互式分析、数据科学、机器学习、实时流处理（Structured Streaming）</td></tr><tr><td align="left"><strong>与对方关系</strong></td><td align="left">可以被Spark SQL作为数据源使用（Spark on Hive）</td><td align="left">可以兼容并加速对Hive表的查询</td></tr></tbody></table><h3 id="场景一：操作-HDFS-上的文件-以CSV文件为例"><a href="#场景一：操作-HDFS-上的文件-以CSV文件为例" class="headerlink" title="场景一：操作 HDFS 上的文件 (以CSV文件为例)"></a><strong>场景一：操作 HDFS 上的文件 (以CSV文件为例)</strong></h3><p>假如，你的数据是以CSV文件的形式存放在HDFS（Hadoop分布式文件系统）中的。</p><h4 id="第1步：读取-Read-HDFS上的CSV文件"><a href="#第1步：读取-Read-HDFS上的CSV文件" class="headerlink" title="第1步：读取 (Read) HDFS上的CSV文件"></a><strong>第1步：读取 (Read) HDFS上的CSV文件</strong></h4><p>假设HDFS上有一个路径为 <code>/user/data/employees.csv</code> 的文件。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这是创建SparkSession的通用代码，我们先要有这个“总指挥官”</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">&quot;HDFSExample&quot;</span>).getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 读取操作 ---</span></span><br><span class="line"><span class="comment"># 命令spark去读取一个csv文件</span></span><br><span class="line"><span class="comment"># .option(&quot;header&quot;, &quot;true&quot;) 表示CSV文件的第一行是列名</span></span><br><span class="line"><span class="comment"># .option(&quot;inferSchema&quot;, &quot;true&quot;) 表示让Spark自动推断列的数据类型（如整数、字符串等）</span></span><br><span class="line"><span class="comment"># .load() 指定文件的HDFS路径</span></span><br><span class="line">df = spark.read.<span class="built_in">format</span>(<span class="string">&quot;csv&quot;</span>) \</span><br><span class="line">              .option(<span class="string">&quot;header&quot;</span>, <span class="string">&quot;true&quot;</span>) \</span><br><span class="line">              .option(<span class="string">&quot;inferSchema&quot;</span>, <span class="string">&quot;true&quot;</span>) \</span><br><span class="line">              .load(<span class="string">&quot;hdfs://&lt;your-namenode-host&gt;:8020/user/data/employees.csv&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为了确认数据已经读进来，我们可以看一下它的结构和前几行</span></span><br><span class="line">df.printSchema() <span class="comment"># 打印表结构</span></span><br><span class="line">df.show(<span class="number">5</span>)       <span class="comment"># 显示前5行数据</span></span><br></pre></td></tr></table></figure><p><strong>代码讲解</strong>：</p><ul><li><code>spark.read</code>：这是告诉Spark我们要开始“读取”数据了。</li><li><code>.format(&quot;csv&quot;)</code>：明确告诉Spark，我们要读的是CSV格式的文件。</li><li><code>.load(&quot;hdfs://...&quot;)</code>：给出文件的具体位置。这就好比告诉快递员要去哪个地址取包裹。</li><li>读取成功后，数据就被装载成一个名为 <code>df</code> 的DataFrame（可以理解为内存中的一张智能表格）。</li></ul><h4 id="第2步：处理-Process-数据"><a href="#第2步：处理-Process-数据" class="headerlink" title="第2步：处理 (Process) 数据"></a><strong>第2步：处理 (Process) 数据</strong></h4><p>现在数据已经在 <code>df</code> 这个DataFrame里了。我们可以用PySpark的函数对它进行各种处理。比如，我们想筛选出所有“IT部门”并且年龄大于30岁的员工。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># --- 处理操作 ---</span></span><br><span class="line"><span class="comment"># .filter() 就是筛选/过滤操作</span></span><br><span class="line">processed_df = df.<span class="built_in">filter</span>((df[<span class="string">&#x27;department&#x27;</span>] == <span class="string">&#x27;IT&#x27;</span>) &amp; (df[<span class="string">&#x27;age&#x27;</span>] &gt; <span class="number">30</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 看一下处理结果</span></span><br><span class="line">processed_df.show()</span><br></pre></td></tr></table></figure><p><strong>代码讲解</strong>：</p><ul><li><code>df.filter(...)</code>：对 <code>df</code> 这张表格执行筛选操作，只保留满足括号内条件的行。简单直观！</li></ul><h4 id="第3步：写回-Write-到HDFS"><a href="#第3步：写回-Write-到HDFS" class="headerlink" title="第3步：写回 (Write) 到HDFS"></a><strong>第3步：写回 (Write) 到HDFS</strong></h4><p>处理完成后，我们想把结果 <code>processed_df</code> 保存回HDFS的一个新位置，比如存成Parquet格式（一种性能更好的列式存储格式）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># --- 写回操作 ---</span></span><br><span class="line"><span class="comment"># .write 是告诉Spark我们要“写入”数据了</span></span><br><span class="line"><span class="comment"># .mode(&quot;overwrite&quot;) 表示如果目标路径已存在，就覆盖它。其他选项有 &quot;append&quot;, &quot;ignore&quot;, &quot;error&quot;</span></span><br><span class="line"><span class="comment"># .save() 指定要保存到的HDFS路径</span></span><br><span class="line">processed_df.write.<span class="built_in">format</span>(<span class="string">&quot;parquet&quot;</span>) \</span><br><span class="line">                  .mode(<span class="string">&quot;overwrite&quot;</span>) \</span><br><span class="line">                  .save(<span class="string">&quot;hdfs://&lt;your-namenode-host&gt;:8020/user/processed_data/it_employees&quot;</span>)</span><br></pre></td></tr></table></figure><p><strong>代码讲解</strong>：</p><ul><li><code>processed_df.write</code>：告诉Spark我们要把 <code>processed_df</code> 这张表的内容写出去。</li><li><code>.format(&quot;parquet&quot;)</code>：指定用Parquet格式保存，这在大数据领域非常流行。</li><li><code>.mode(&quot;overwrite&quot;)</code>：这是一个非常重要的写入模式设置，<code>overwrite</code>会覆盖已有数据，非常适合重复运行的脚本。</li><li><code>.save(&quot;hdfs://...&quot;)</code>：给出保存包裹的目标地址。</li></ul><hr><h3 id="场景二：操作-Hive-仓库中的表"><a href="#场景二：操作-Hive-仓库中的表" class="headerlink" title="场景二：操作 Hive 仓库中的表"></a><strong>场景二：操作 Hive 仓库中的表</strong></h3><p>现在，假设你的数据源是Hive里的一张表，而不是HDFS上的一个独立文件。</p><h4 id="第1步：读取-Read-Hive表"><a href="#第1步：读取-Read-Hive表" class="headerlink" title="第1步：读取 (Read) Hive表"></a><strong>第1步：读取 (Read) Hive表</strong></h4><p>在连接Hive之前，创建<code>SparkSession</code>时需要加一点“佐料”：<code>.enableHiveSupport()</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建支持Hive的SparkSession</span></span><br><span class="line"><span class="comment"># 这里的 .enableHiveSupport() 是关键！</span></span><br><span class="line">spark_hive = SparkSession.builder.appName(<span class="string">&quot;HiveExample&quot;</span>) \</span><br><span class="line">                           .enableHiveSupport() \</span><br><span class="line">                           .getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 读取操作 ---</span></span><br><span class="line"><span class="comment"># 假设Hive里有一个数据库叫 `my_db`，里面有张表叫 `employees`</span></span><br><span class="line"><span class="comment"># 使用 .table() 方法直接读取，非常方便</span></span><br><span class="line">hive_df = spark_hive.read.table(<span class="string">&quot;my_db.employees&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 或者，你也可以用SparkSQL来做同样的事情，效果完全一样</span></span><br><span class="line"><span class="comment"># 这就是SparkSQL的威力！</span></span><br><span class="line">hive_df_sql = spark_hive.sql(<span class="string">&quot;SELECT * FROM my_db.employees&quot;</span>)</span><br><span class="line"></span><br><span class="line">hive_df.show(<span class="number">5</span>)</span><br></pre></td></tr></table></figure><p><strong>代码讲解</strong>：</p><ul><li><code>.enableHiveSupport()</code>：这个命令让<code>SparkSession</code>能够连接到Hive的元数据仓库（Metastore），从而知道<code>my_db.employees</code>这张表在哪里、长什么样。</li><li><code>spark_hive.read.table(&quot;my_db.employees&quot;)</code>：这是读取Hive表最直接的方式。因为Spark已经通过元数据知道了表的一切信息，所以你只需要提供表名即可。</li></ul><h4 id="第2步：处理-Process-数据-1"><a href="#第2步：处理-Process-数据-1" class="headerlink" title="第2步：处理 (Process) 数据"></a><strong>第2步：处理 (Process) 数据</strong></h4><p>处理方式和操作HDFS文件读进来的DataFrame完全一样！比如，我们按部门计算平均工资。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># --- 处理操作 ---</span></span><br><span class="line"><span class="comment"># .groupBy() 是分组操作，.agg() 是聚合操作</span></span><br><span class="line"><span class="comment"># 这里计算每个部门(department)的平均工资(salary)</span></span><br><span class="line">salary_summary_df = hive_df.groupBy(<span class="string">&quot;department&quot;</span>) \</span><br><span class="line">                           .agg(&#123;<span class="string">&quot;salary&quot;</span>: <span class="string">&quot;avg&quot;</span>&#125;) \</span><br><span class="line">                           .withColumnRenamed(<span class="string">&quot;avg(salary)&quot;</span>, <span class="string">&quot;average_salary&quot;</span>) <span class="comment"># 给计算出的列改个好听的名字</span></span><br><span class="line"></span><br><span class="line">salary_summary_df.show()</span><br></pre></td></tr></table></figure><p><strong>代码讲解</strong>：</p><ul><li><code>groupBy(&quot;department&quot;)</code>：按“部门”列进行分组。</li><li><code>.agg({&quot;salary&quot;: &quot;avg&quot;})</code>：对每个分组后的数据，计算“salary”列的平均值（avg）。</li></ul><h4 id="第3步：写回-Write-到Hive"><a href="#第3步：写回-Write-到Hive" class="headerlink" title="第3步：写回 (Write) 到Hive"></a><strong>第3步：写回 (Write) 到Hive</strong></h4><p>我们可以将处理结果 <code>salary_summary_df</code> 保存为一张新的Hive表。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># --- 写回操作 ---</span></span><br><span class="line"><span class="comment"># .saveAsTable() 可以将DataFrame直接保存为一张Hive表</span></span><br><span class="line">salary_summary_df.write.mode(<span class="string">&quot;overwrite&quot;</span>).saveAsTable(<span class="string">&quot;my_db.salary_summary&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 你可以立即用SQL查询这张新表，验证写入是否成功</span></span><br><span class="line">spark_hive.sql(<span class="string">&quot;SELECT * FROM my_db.salary_summary&quot;</span>).show()</span><br></pre></td></tr></table></figure><p><strong>代码讲解</strong>：</p><ul><li><code>.saveAsTable(&quot;my_db.salary_summary&quot;)</code>：这是将DataFrame写入为Hive表的核心命令。Spark会自动处理底层的HDFS文件存储和Hive元数据更新，非常省心。</li></ul><hr><p><strong>总结一下</strong>：</p><ul><li><strong>对HDFS文件</strong>：<code>spark.read.format(...).load(&quot;路径&quot;)</code> -&gt; 处理 -&gt; <code>df.write.format(...).save(&quot;路径&quot;)</code></li><li><strong>对Hive表</strong>：<code>spark.read.table(&quot;库.表&quot;)</code> 或 <code>spark.sql(&quot;SELECT...&quot;)</code> -&gt; 处理 -&gt; <code>df.write.saveAsTable(&quot;库.表&quot;)</code></li><li><strong>关键区别</strong>：操作Hive需要<code>enableHiveSupport()</code>，并且读写时用的是<strong>表名</strong>，而操作HDFS用的是<strong>文件路径</strong>。</li></ul><h2 id="核心概念"><a href="#核心概念" class="headerlink" title="核心概念"></a>核心概念</h2><h3 id="Driver"><a href="#Driver" class="headerlink" title="Driver"></a>Driver</h3><p>接到需求后，进行规划，制定计划。一个计划由多个阶段（Stage）组成，分阶段的依据是 是否需要进行 Shuffle</p><h3 id="Stage"><a href="#Stage" class="headerlink" title="Stage"></a>Stage</h3><p>一个Stage会有很多可以同时进行的、性质一样的小任务（Task）</p><h3 id="Shuffle"><a href="#Shuffle" class="headerlink" title="Shuffle"></a>Shuffle</h3><p>发生在两个有依赖关系的Stage之间</p><h3 id="Executor"><a href="#Executor" class="headerlink" title="Executor"></a>Executor</h3><p>真正执行任务的进程，会去Driver那里领取任务</p><h3 id="task"><a href="#task" class="headerlink" title="task"></a>task</h3><p>最小执行单元</p>]]></content>
      
      
      
        <tags>
            
            <tag> Spark </tag>
            
            <tag> 数据开发 </tag>
            
            <tag> SQL </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>VS Code 遇到的 No module named 问题</title>
      <link href="/2026/01/22/vscode_and_python/"/>
      <url>/2026/01/22/vscode_and_python/</url>
      
        <content type="html"><![CDATA[<p>VS Code 这个“万金油”是我非常喜欢的编辑器，然而在用它写 Python 项目时，经常会出现 <code>ModuleNotFoundError: No module named &#39;xxx&#39;</code> 的情况。</p><p>这里的 <code>xxx</code> 一般是自己写的模块（第三方库通常已在 Python 默认路径中）。出现这个问题，通常是因为 <strong>Python 解释器不知道去哪里找你写的代码</strong>。</p><h3 id="问题场景"><a href="#问题场景" class="headerlink" title="问题场景"></a>问题场景</h3><p>假设你的项目目录结构如下：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ProjectRoot/</span><br><span class="line">├── utils/</span><br><span class="line">│   └── helper.py</span><br><span class="line">└── src/</span><br><span class="line">    └── main.py</span><br></pre></td></tr></table></figure><p>当你在 <code>ProjectRoot</code> 根目录下打开 VS Code，并在终端运行 <code>src/main.py</code> 时，如果不做配置，Python 往往无法在 <code>main.py</code> 中引用 <code>utils.helper</code>，因为它找不到 <code>utils</code> 文件夹。</p><h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><h4 id="方法一：修改-settings-json（亲测有效）"><a href="#方法一：修改-settings-json（亲测有效）" class="headerlink" title="方法一：修改 settings.json（亲测有效）"></a>方法一：修改 settings.json（亲测有效）</h4><p>这是一个简单粗暴且有效的方法。通过配置 VS Code 的终端环境，强制将项目根目录加入到 Python 的搜索路径中。<br><strong>步骤：</strong></p><ol><li><p><code>Ctrl + Shift + P</code> 打开命令面板，输入 <code>Preferences: Open Settings (JSON)</code>（首选项：打开设置(JSON)）。<br><img src="/images/vscode%E6%89%93%E5%BC%80setting-json.png" alt="图片" title="Setting.json"></p></li><li><p>在 JSON 文件的大括号 <code>{ ... }</code> 中加入以下配置：<br><img src="/images/python%E5%8A%A0%E5%85%A5%E5%B7%A5%E4%BD%9C%E5%8C%BA.png" alt="图片" title="Setting-2.json"></p></li></ol><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">&quot;terminal.integrated.env.osx&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;PYTHONPATH&quot;</span><span class="punctuation">:</span> <span class="string">&quot;$&#123;workspaceFolder&#125;&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line"><span class="attr">&quot;terminal.integrated.env.linux&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;PYTHONPATH&quot;</span><span class="punctuation">:</span> <span class="string">&quot;$&#123;workspaceFolder&#125;&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line"><span class="attr">&quot;terminal.integrated.env.windows&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;PYTHONPATH&quot;</span><span class="punctuation">:</span> <span class="string">&quot;$&#123;workspaceFolder&#125;&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><p><strong>原理（先来段AI解释）：</strong></p><ol><li><p><strong>配置项作用</strong>：</p><ul><li><code>terminal.integrated.env.*</code>: 针对不同操作系统（macOS&#x2F;Linux&#x2F;Windows），为 VS Code 的<strong>集成终端</strong>设置环境变量。</li><li>这确保了无论你在什么系统上开发，只要在 VS Code 终端里跑 Python，这个环境变量都会生效。</li></ul></li><li><p><strong>设置的内容</strong>：</p><ul><li><code>&quot;PYTHONPATH&quot;: &quot;${workspaceFolder}&quot;</code>：<ul><li><code>PYTHONPATH</code>：这是一个经典的环境变量。Python 解释器在启动时，会先把这个变量里指定的路径加入到搜索列表中。</li><li><code>&quot;${workspaceFolder}&quot;</code>：这是 VS Code 的预定义变量，代表<strong>当前打开项目的根目录绝对路径</strong>。</li></ul></li></ul></li></ol><p><strong>简单来说（人话）</strong>：就是让Python把我整个项目的根目录（其实是vscode打开的目录）也作为搜索代码的地方</p><hr><h4 id="方法二：使用-env-文件（这个没试过）"><a href="#方法二：使用-env-文件（这个没试过）" class="headerlink" title="方法二：使用 .env 文件（这个没试过）"></a>方法二：使用 .env 文件（这个没试过）</h4><p>如果补想污染全局配置，或者希望配置能跟随项目代码（Git）走，可以使用 <code>.env</code> 文件。VS Code 的 Python 插件会自动读取该文件。</p><ol><li>在项目根目录下创建一个名为 <code>.env</code> 的文件。</li><li>在里面写入：<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">PYTHONPATH=.</span><br></pre></td></tr></table></figure></li><li>确保你的 <code>settings.json</code> 中有如下配置（默认通常就是开启的）：<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">&quot;python.envFile&quot;</span><span class="punctuation">:</span> <span class="string">&quot;$&#123;workspaceFolder&#125;/.env&quot;</span></span><br></pre></td></tr></table></figure>这样不仅终端运行有效，调试器（Debugger）启动时也会自动生效。</li></ol><hr><h3 id="Python-的搜寻路径-原理深入"><a href="#Python-的搜寻路径-原理深入" class="headerlink" title="Python 的搜寻路径 (原理深入)"></a>Python 的搜寻路径 (原理深入)</h3><p>为什么会出现这个问题？这需要理解 Python 查找模块的逻辑（<code>sys.path</code>）。</p><p>当你执行 <code>python src/main.py</code> 时，Python 的搜索路径顺序通常如下：</p><ol><li><strong>当前脚本所在的目录</strong>：即 <code>src/</code> 目录。（注意：<strong>不是</strong>你运行命令的 <code>ProjectRoot</code> 目录！）</li><li><strong>PYTHONPATH 环境变量</strong>：我们在上面配置的就是这一项。</li><li><strong>标准库目录</strong>：安装 Python 时自带的库（如 <code>os</code>, <code>sys</code> 等）。</li><li><strong>第三方库目录 (site-packages)</strong>：你用 <code>pip install</code> 安装的库都在这里。</li></ol><p><strong>问题根源：</strong><br>因为默认情况下，Python 只把 <code>src/</code> 加入了路径。当你代码里写 <code>from utils import helper</code> 时，Python 在 <code>src/</code> 目录下找不到 <code>utils</code> 文件夹（因为它在上一级），于是就报错 <code>No module named</code>。</p><p>通过设置 <code>PYTHONPATH</code> 为项目根目录，Python 就能顺利在根目录下找到 <code>utils</code> 文件夹了。</p>]]></content>
      
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> vscode </tag>
            
            <tag> 经验分享 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python的__init__文件</title>
      <link href="/2026/01/21/python-1/"/>
      <url>/2026/01/21/python-1/</url>
      
        <content type="html"><![CDATA[<h2 id="1-init-py-的核心作用"><a href="#1-init-py-的核心作用" class="headerlink" title="1. __init__.py 的核心作用"></a>1. <code>__init__.py</code> 的核心作用</h2><p>在 Python 中，包含 <code>__init__.py</code> 文件的目录被视为一个 <strong>Package（包）</strong>。</p><ul><li><strong>标识作用</strong>：告诉 Python 解释器这个目录不仅仅是文件夹，而是一个可导入的包。</li><li><strong>初始化代码</strong>：当包被导入时，<code>__init__.py</code> 中的代码会自动执行（且仅执行一次）。</li></ul><hr><h2 id="2-all-详解"><a href="#2-all-详解" class="headerlink" title="2. __all__ 详解"></a>2. <code>__all__</code> 详解</h2><p><code>__all__</code> 是一个字符串列表，用于控制 <code>from package import *</code> 的行为。</p><ul><li><strong>作用</strong>：当你使用 <code>from package import *</code> 时，只有在 <code>__all__</code> 列表中列出的变量、函数或类会被导入到当前的命名空间。</li><li><strong>最佳实践</strong>：如果不定义 <code>__all__</code>，默认会导入模块中所有不以下划线 <code>_</code> 开头的名称。显式定义 <code>__all__</code> 可以避免污染用户的命名空间，隐藏内部实现细节。</li></ul><p><strong>示例：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 文件：mypackage/__init__.py</span></span><br><span class="line"></span><br><span class="line">__all__ = [<span class="string">&#x27;PublicClass&#x27;</span>, <span class="string">&#x27;public_func&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PublicClass</span>:</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">public_func</span>():</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_internal_func</span>():</span><br><span class="line">    <span class="comment"># 这个函数不会被 &#x27;from mypackage import *&#x27; 导入</span></span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><hr><h2 id="3-Import-功能详解：如何设计优雅的-API接口"><a href="#3-Import-功能详解：如何设计优雅的-API接口" class="headerlink" title="3. Import 功能详解：如何设计优雅的 API接口"></a>3. Import 功能详解：如何设计优雅的 API接口</h2><p><em>(参考自 Geek-Docs: How do I write good&#x2F;correct package <strong>init</strong>.py files)</em></p><p><code>__init__.py</code> 最强大的功能之一是<strong>组织和简化包的导入接口</strong>。通过在 <code>__init__.py</code> 中精心设计 import 语句，可以向用户提供更友好的访问方式。</p><h3 id="A-提升命名空间-Hoisting-便捷导入"><a href="#A-提升命名空间-Hoisting-便捷导入" class="headerlink" title="A. 提升命名空间 (Hoisting) &#x2F; 便捷导入"></a>A. 提升命名空间 (Hoisting) &#x2F; 便捷导入</h3><p>通常，我们的代码逻辑分散在包内的不同子模块中。如果不处理，用户导入时需要写很长的路径。我们可以在 <code>__init__.py</code> 中将核心类或函数导入，使其对外暴露在包的顶层。</p><p><strong>场景假设：</strong><br>目录结构如下：</p><figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mypackage/</span><br><span class="line">    ├── __init__.py</span><br><span class="line">    ├── database.py  &lt;-- 里面定义了 Database 类</span><br><span class="line">    └── network.py   &lt;-- 里面定义了 Connect 函数</span><br></pre></td></tr></table></figure><p><strong>不好的体验（如果不配置 <code>__init__.py</code>）：</strong><br>用户必须这样写：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> mypackage.database</span><br><span class="line"><span class="keyword">import</span> mypackage.network</span><br><span class="line"></span><br><span class="line">db = mypackage.database.Database()</span><br><span class="line">conn = mypackage.network.Connect()</span><br></pre></td></tr></table></figure><p><strong>优雅的体验（在 <code>__init__.py</code> 中配置）：</strong></p><p>在 <code>mypackage/__init__.py</code> 中写入：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用相对导入，将子模块的内容引入到当前包的命名空间</span></span><br><span class="line"><span class="keyword">from</span> .database <span class="keyword">import</span> Database</span><br><span class="line"><span class="keyword">from</span> .network <span class="keyword">import</span> Connect</span><br><span class="line"></span><br><span class="line"><span class="comment"># 配合 __all__ 使用，明确导出的接口</span></span><br><span class="line">__all__ = [<span class="string">&#x27;Database&#x27;</span>, <span class="string">&#x27;Connect&#x27;</span>]</span><br></pre></td></tr></table></figure><p><strong>用户现在可以这样写：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> mypackage</span><br><span class="line"></span><br><span class="line"><span class="comment"># 直接通过包名访问，无需知道内部文件结构</span></span><br><span class="line">db = mypackage.Database()</span><br><span class="line">conn = mypackage.Connect()</span><br></pre></td></tr></table></figure><h3 id="B-区分“接口”与“实现”"><a href="#B-区分“接口”与“实现”" class="headerlink" title="B. 区分“接口”与“实现”"></a>B. 区分“接口”与“实现”</h3><p>通过在 <code>__init__.py</code> 中控制 import，你可以重构内部文件的结构（例如把一个大文件拆分成五个小文件），但只要 <code>__init__.py</code> 暴露的接口不变，使用你代码的用户就不需要修改任何代码。</p><h3 id="C-处理包级别的初始化"><a href="#C-处理包级别的初始化" class="headerlink" title="C. 处理包级别的初始化"></a>C. 处理包级别的初始化</h3><p>你可以在 <code>__init__.py</code> 中导入必要的依赖或设置全局配置。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># mypackage/__init__.py</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查环境变量配置</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.getenv(<span class="string">&quot;API_KEY&quot;</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Warning: API_KEY not set&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 暴露核心类</span></span><br><span class="line"><span class="keyword">from</span> .core <span class="keyword">import</span> MainService</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
