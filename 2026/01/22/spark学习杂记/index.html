
<!DOCTYPE html><html lang="zh-CN">

<head>
  <meta charset="utf-8">
  <meta name="hexo-theme" content="https://github.com/xaoxuu/hexo-theme-stellar/tree/1.33.1" theme-name="Stellar" theme-version="1.33.1">
  
  
  <meta name="generator" content="Hexo 8.1.1">
  <meta http-equiv='x-dns-prefetch-control' content='on' />
  
  <meta name="renderer" content="webkit">
  <meta name="force-rendering" content="webkit">
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
  <meta name="HandheldFriendly" content="True" >
  <meta name="mobile-web-app-capable" content="yes">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="theme-color" media="(prefers-color-scheme: dark)" content="#000">
  <meta name="theme-color" content="#f9fafb">
  <title>spark学习杂记 - Love Code（爱扣)</title>

  
    <meta name="description" content="Spark的核心概念1.RDD–&gt;Spark的核心基石RDD 的全称是 Resilient Distributed Dataset，中文是“弹性分布式数据集”。我们把它拆开来看：  Dataset (数据集)：它就是一个集合，里面存放着你的数据。 Distributed (分布式)：这个数据集不是存放在一台机器上，而是被切分成很多小块（称为“分区”），分散存储在集群的多台机器上。这样做的好处">
<meta property="og:type" content="article">
<meta property="og:title" content="spark学习杂记">
<meta property="og:url" content="https://prose2077.github.io/2026/01/22/spark%E5%AD%A6%E4%B9%A0%E6%9D%82%E8%AE%B0/">
<meta property="og:site_name" content="Love Code（爱扣)">
<meta property="og:description" content="Spark的核心概念1.RDD–&gt;Spark的核心基石RDD 的全称是 Resilient Distributed Dataset，中文是“弹性分布式数据集”。我们把它拆开来看：  Dataset (数据集)：它就是一个集合，里面存放着你的数据。 Distributed (分布式)：这个数据集不是存放在一台机器上，而是被切分成很多小块（称为“分区”），分散存储在集群的多台机器上。这样做的好处">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://free.picui.cn/free/2026/01/22/6971f83dc1c04.png">
<meta property="article:published_time" content="2026-01-22T13:07:25.000Z">
<meta property="article:modified_time" content="2026-01-23T02:37:33.611Z">
<meta property="article:author" content="我们">
<meta property="article:tag" content="Python">
<meta property="article:tag" content="SQL">
<meta property="article:tag" content="Spark">
<meta property="article:tag" content="数据开发">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://free.picui.cn/free/2026/01/22/6971f83dc1c04.png">
  
  
  
  <meta name="keywords" content="Spark,数据开发,SQL,Python">

  <!-- feed -->
  
    <link rel="alternate" href="/atom.xml" title="Love Code（爱扣)" type="application/atom+xml">
  

  <link rel="stylesheet" href="/css/main.css?v=1.33.1">


  

  

  <script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"我们","sameAs":[],"image":"https://free.picui.cn/free/2026/01/22/6971f83dc1c04.png"},"dateCreated":"2026-01-22T21:07:25+08:00","dateModified":"2026-01-23T10:37:33+08:00","datePublished":"2026-01-22T21:07:25+08:00","description":"","headline":"spark学习杂记","mainEntityOfPage":{"@type":"WebPage","@id":"https://prose2077.github.io/2026/01/22/spark%E5%AD%A6%E4%B9%A0%E6%9D%82%E8%AE%B0/"},"publisher":{"@type":"Organization","name":"我们","sameAs":[],"image":"https://free.picui.cn/free/2026/01/22/6971f83dc1c04.png","logo":{"@type":"ImageObject","url":"https://free.picui.cn/free/2026/01/22/6971f83dc1c04.png"}},"url":"https://prose2077.github.io/2026/01/22/spark%E5%AD%A6%E4%B9%A0%E6%9D%82%E8%AE%B0/","keywords":"Spark, 数据开发, SQL, Python","image":[]}</script>
  
</head>
<body>

<div class="l_body content" id="start" layout="post" type="tech" ><aside class="l_left"><div class="sidebg"></div><div class="leftbar-container">


<header class="header"><div class="logo-wrap"><a class="avatar" href="/about/"><div class="bg" style="opacity:0;background-image:url(https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.12/avatar/round/rainbow64@3x.webp);"></div><img no-lazy class="avatar" src="https://free.picui.cn/free/2026/01/22/6971f83dc1c04.png" onerror="javascript:this.classList.add('error');this.src='https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.12/image/2659360.svg';"></a><a class="title" href="/"><div class="main">Love Code（爱扣)</div></a></div></header>

<div class="nav-area">

<nav class="menu dis-select"></nav>
</div>
<div class="widgets">
<div class="search-wrapper" id="search-wrapper"><form class="search-form"><a class="search-button" onclick="document.getElementById(&quot;search-input&quot;).focus();"><svg t="1705074644177" viewBox="0 0 1025 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="1560" width="200" height="200"><path d="M1008.839137 935.96571L792.364903 719.491476a56.783488 56.783488 0 0 0-80.152866 0 358.53545 358.53545 0 1 1 100.857314-335.166073 362.840335 362.840335 0 0 1-3.689902 170.145468 51.248635 51.248635 0 1 0 99.217358 26.444296 462.057693 462.057693 0 1 0-158.255785 242.303546l185.930047 185.725053a51.248635 51.248635 0 0 0 72.568068 0 51.248635 51.248635 0 0 0 0-72.978056z" p-id="1561"></path><path d="M616.479587 615.969233a50.428657 50.428657 0 0 0-61.498362-5.534852 174.655348 174.655348 0 0 1-177.525271 3.484907 49.403684 49.403684 0 0 0-58.833433 6.76482l-3.074918 2.869923a49.403684 49.403684 0 0 0 8.609771 78.10292 277.767601 277.767601 0 0 0 286.992355-5.739847 49.403684 49.403684 0 0 0 8.404776-76.667958z" p-id="1562"></path></svg></a><input type="text" class="search-input" id="search-input" placeholder="站内搜索"></form><div id="search-result"></div><div class="search-no-result">没有找到内容！</div></div>



<widget class="widget-wrapper recent post-list"><div class="widget-header dis-select"><span class="name">最近更新</span></div><div class="widget-body fs14"><a class="item title" href="/2026/01/23/vscode-copilot%E6%8E%A5%E7%AC%AC%E4%B8%89%E6%96%B9api/"><span class="title">vscode_copilot接第三方api</span></a><a class="item title" href="/2026/01/22/spark%E5%AD%A6%E4%B9%A0%E6%9D%82%E8%AE%B0/"><span class="title">spark学习杂记</span></a><a class="item title" href="/2026/01/22/vscode_and_python/"><span class="title">VS Code 遇到的 No module named 问题</span></a><a class="item title" href="/2026/01/21/python-1/"><span class="title">python的__init__文件</span></a></div></widget>
</div>

</div></aside><div class="l_main" id="main">





<div class="article banner top">
  <div class="content">
    <div class="top bread-nav footnote"><div class="left"><div class="flex-row" id="breadcrumb"><a class="cap breadcrumb" href="/">主页</a>
<span class="sep"></span><a class="cap breadcrumb" href="/">文章</a></div>
<div class="flex-row" id="post-meta"><a class="author" href="/author/cdc/">Chen Dongcheng (Cdc)</a><span class="sep"></span><span class="text created"><time datetime="2026-01-22T13:07:25.000Z">2026-01-22</time></span><span class="sep updated"></span><span class="text updated">更新于：<time datetime="2026-01-23T02:37:33.611Z">2026-01-23</time></span></div></div></div>
    
    <div class="bottom only-title">
      
      <div class="text-area">
        <h1 class="text title"><span>spark学习杂记</span></h1>
        
      </div>
    </div>
    
  </div>
  </div><article class="md-text content"><h2 id="Spark的核心概念"><a href="#Spark的核心概念" class="headerlink" title="Spark的核心概念"></a>Spark的核心概念</h2><h3 id="1-RDD–-Spark的核心基石"><a href="#1-RDD–-Spark的核心基石" class="headerlink" title="1.RDD–&gt;Spark的核心基石"></a>1.RDD–&gt;Spark的核心基石</h3><p><strong>RDD</strong> 的全称是 <strong>Resilient Distributed Dataset</strong>，中文是“<strong>弹性分布式数据集</strong>”。我们把它拆开来看：</p>
<ul>
<li><strong>Dataset (数据集)</strong>：它就是一个集合，里面存放着你的数据。</li>
<li><strong>Distributed (分布式)</strong>：这个数据集不是存放在一台机器上，而是被切分成很多小块（称为“分区”），分散存储在集群的多台机器上。这样做的好处是，可以利用多台机器的计算能力并行处理这些数据，大大提升了速度。</li>
<li><strong>Resilient (弹性的&#x2F;可容错的)</strong>：这是 RDD 最核心的特性。如果存储某个数据分区的机器宕机了，数据丢失了怎么办？没关系，Spark 会根据数据的“血缘关系”（Lineage）——也就是记录了这个数据分区是通过哪些计算步骤得来的——自动在另一台正常的机器上重新计算出丢失的数据。因此，它具有很强的容错能力，像一个不倒翁。</li>
</ul>
<p>简单总结：<strong>RDD 是一个被分区、不可变、支持并行操作的分布式数据集合，并且拥有强大的自动容错能力。</strong></p>
<p>RDD的特性如下：</p>
<ol>
<li>不可变性：RDD 一旦被创建就不能被修改。如果想“修改”它，你只能通过一个“转换操作 (Transformation)”来生成一个<strong>新的 RDD</strong>。</li>
<li>惰性：当你对一个 RDD 执行转换操作时（比如 <code>map</code>, <code>filter</code>），Spark 并<strong>不会立即计算</strong>。它只是默默地记下这个操作，把它加入到“血缘关系”的图（DAG）中。只有当一个“行动操作 (Action)”被触发时（比如 <code>count</code>, <code>collect</code>, <code>save</code>），Spark 才会真正开始执行从头到尾的所有计算。</li>
<li>血缘（DAG）：也称为 DAG (Directed Acyclic Graph, 有向无环图)。这是 RDD 容错机制的核心，就像“乐高说明书”。它记录了数据从源头到最终结果的每一步转换。</li>
</ol>
<h3 id="2-Spark的主流API-dataFrame"><a href="#2-Spark的主流API-dataFrame" class="headerlink" title="2.Spark的主流API-dataFrame"></a>2.Spark的主流API-dataFrame</h3><p><strong>DataFrame</strong>，中文是“数据帧”。它本质上也是一个分布式的数据集合，但它更像我们非常熟悉的<strong>关系型数据库中的表 (Table)</strong> 或者一个 <strong>Excel 电子表格</strong>。</p>
<p>与 RDD 不同的是，DataFrame 的数据是<strong>结构化的</strong>，它有明确的<strong>列名 (Columns)</strong> 和每一列的数据<strong>类型 (Schema)</strong>。</p>
<h3 id="3-DataSet-DataFrame的扩展"><a href="#3-DataSet-DataFrame的扩展" class="headerlink" title="3.DataSet -&gt; DataFrame的扩展"></a>3.DataSet -&gt; DataFrame的扩展</h3><p><strong>DataSet</strong> 是在 Spark 1.6 中引入的，可以看作是 DataFrame 的一个<strong>扩展</strong>。它试图结合 DataFrame 的性能优势和 RDD 的类型安全特性。</p>
<ul>
<li><p><strong>在 Java&#x2F;Scala 中</strong>：DataSet 是一个<strong>强类型</strong>的 API。这意味着你在编译时就能检查出类型错误。例如，你定义一个 <code>DataSet[Person]</code>，那么集合里的每个元素都必须是 <code>Person</code> 对象，如果你试图往里面放一个 <code>Car</code> 对象，编译器会直接报错。这让代码更健壮。</p>
</li>
<li><p><strong>在 Python (PySpark) 中</strong>：这是一个<strong>非常非常重要</strong>的点！<strong>Python 语言本身是动态类型的，所以 PySpark 中没有一个独立的、像 Java&#x2F;Scala 中那样强类型的 DataSet API。在 PySpark 里，DataFrame 就是 DataSet 的一个特例，可以认为 <code>DataFrame = DataSet[Row]</code></strong>。Row 是一个通用的行对象。</p>
</li>
</ul>
<p><em><strong>对于Python而言</strong></em>：DataSet就是DataFrame</p>
<h2 id="如何理解Spark相比于MapReduce的优势？"><a href="#如何理解Spark相比于MapReduce的优势？" class="headerlink" title="如何理解Spark相比于MapReduce的优势？"></a>如何理解Spark相比于MapReduce的优势？</h2><h3 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h3><p><strong>MapReduce</strong> 的核心思想其实就是八个字：<strong>分而治之，先分后总</strong><br>假设需要统计一个图书馆的每个单词各自出现了多少次。一个人肯定干不完，就会采用MapReduce的思想：</p>
<ol>
<li><p><strong>Map (映射&#x2F;分工)</strong>: 您把图书馆的书分成很多份，找来一大群图书管理员（比如100个），每人分几架书。 您给他们下达一个<strong>完全相同</strong>的简单指令：“你们每个人，把自己负责的书从头到尾读一遍，每遇到一个单词，就在一张小卡片上写下 <code>(这个单词, 1)</code>。比如读到’Hello’，就写一张 <code>(Hello, 1)</code> 的卡片。” 这个过程就是 <strong>Map</strong>。它的本质是把一个大任务，<strong>切分</strong>成无数个可以<strong>并行处理</strong>的小任务，并把原始数据处理成一种统一的格式（这里就是 <code>(单词, 1)</code> 这种键值对）。</p>
</li>
<li><p><strong>Reduce (归约&#x2F;汇总)</strong>: 现在，您面前堆满了山一样的小卡片。您又找来另一批管理员。 您让他们先把所有卡片按照单词的首字母排序、整理。把所有写着 <code>(Hello, 1)</code> 的卡片放在一起，所有 <code>(World, 1)</code> 的卡片放在一起，以此类推。 然后，您给每个单词指派一位“汇总员”。负责’Hello’的汇总员，只需要数一下他面前有多少张 <code>(Hello, 1)</code> 的卡片，比如有5000张，他最后就向您汇报一个结果：<code>(Hello, 5000)</code>。 这个过程就是 <strong>Reduce</strong>。它的本质是对Map阶段处理好的、已经按“键”（Key，这里指单词）分类的数据进行<strong>汇总</strong>和<strong>计算</strong>，得出最终结果。</p>
</li>
</ol>
<p>MapReduce的中间结果，也就是上面例子的小卡片，是需要存入硬盘的。假设上面例子的图书馆，它的地面是<strong>硬盘（HDFS）</strong>。</p>
<ul>
<li>当Map阶段的图书管理员们写好 <code>(单词, 1)</code> 的小卡片后，他们必须把这些卡片<strong>扔在地上（写入硬盘）</strong>。</li>
<li>然后，系统要等所有人都扔完，再去地上把所有卡片捡起来、分类、排序。</li>
<li>最后，Reduce阶段的汇总员们，再从地上<strong>捡起</strong>属于自己的那一堆卡片（比如所有’Hello’的卡片），进行汇总。</li>
</ul>
<h3 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h3><p>Spark是基于内存计算的，回到上面的例子，这个“扔在地上”再“捡起来”的动作，也就是**“写入硬盘”<strong>和</strong>“读取硬盘”**，是非常耗费时间的。</p>
<p>在Spark的世界里，图书馆里配备了许多张巨大的<strong>工作台（内存）</strong>。</p>
<ul>
<li>Map阶段的管理员们写好 <code>(单词, 1)</code> 的卡片后，不再扔到地上，而是整齐地摆在自己的工作台上。</li>
<li>当需要进行下一步（比如Reduce）时，汇总员们可以直接从这些工作台上拿走卡片进行处理，速度飞快。</li>
</ul>
<p>所以，Spark相比于MapReduce最核心的优势就是：<strong>它尽可能地将计算过程中的中间数据存放在速度极快的内存中，而不是每次都写入缓慢的硬盘，从而极大地提升了计算效率。</strong></p>
<p><em><strong>简而言之，Spark 通过内存计算，避开了MapReduce昂贵的磁盘I&#x2F;O开销</strong></em></p>
<h3 id="如何使用-PySpark-DataFrame-API-从-HDFS-读取文件并进行操作"><a href="#如何使用-PySpark-DataFrame-API-从-HDFS-读取文件并进行操作" class="headerlink" title="如何使用 PySpark DataFrame API 从 HDFS 读取文件并进行操作"></a>如何使用 PySpark DataFrame API 从 HDFS 读取文件并进行操作</h3><p>主要分3步：</p>
<ol>
<li>建立连接（<strong><code>SparkSession</code></strong>）：您、需要创建一个<code>SparkSession</code>对象，它是程序和Spark集群沟通的唯一入口。</li>
<li><strong>下达读取指令 (<code>spark.read</code>)</strong>: 有了指挥部，您就可以下达指令了。<code>spark.read</code> 就是“读取”这个动作的开始。</li>
<li><strong>指定目标和格式 (<code>.format().load()</code>)</strong>: 您需要告诉士兵们：<ul>
<li>要读取的文件是什么<strong>格式</strong>？（例如是CSV, JSON, 还是Parquet）—— 这通过 <code>.format(&quot;csv&quot;)</code> 来指定。</li>
<li>文件具体放在仓库的<strong>哪个位置</strong>？—— 这通过 <code>.load(&quot;hdfs://...&quot;)</code> 来指定。HDFS的路径通常以 <code>hdfs://</code> 开头。<br>下面是描述这个过程的伪代码：</li>
</ul>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. 导入并创建 SparkSession (你的“指挥部”)</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">&quot;ReadHDFSExample&quot;</span>).getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 定义 HDFS 上的文件路径</span></span><br><span class="line"><span class="comment"># 这里的 &lt;namenode_host&gt;:&lt;port&gt; 是你HDFS主节点的地址和端口</span></span><br><span class="line">hdfs_path = <span class="string">&quot;hdfs://&lt;namenode_host&gt;:&lt;port&gt;/path/to/your/file.csv&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 使用 DataFrame API 读取文件</span></span><br><span class="line"><span class="comment"># .format(&quot;csv&quot;) -&gt; 指定文件格式是CSV</span></span><br><span class="line"><span class="comment"># .option(&quot;header&quot;, &quot;true&quot;) -&gt; 告诉Spark第一行是表头，而不是数据</span></span><br><span class="line"><span class="comment"># .load(hdfs_path) -&gt; 从指定HDFS路径加载数据</span></span><br><span class="line">df = spark.read.<span class="built_in">format</span>(<span class="string">&quot;csv&quot;</span>) \</span><br><span class="line">            .option(<span class="string">&quot;header&quot;</span>, <span class="string">&quot;true&quot;</span>) \</span><br><span class="line">            .load(hdfs_path)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 对创建的 DataFrame 进行一个简单操作，比如打印它的结构</span></span><br><span class="line">df.printSchema()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. 显示前几行数据，确认读取成功</span></span><br><span class="line">df.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6. 任务完成，关闭指挥部，释放资源</span></span><br><span class="line">spark.stop()</span><br></pre></td></tr></table></figure>


<h3 id="SparkSQL"><a href="#SparkSQL" class="headerlink" title="SparkSQL"></a>SparkSQL</h3><p><strong>Spark SQL</strong> 是Apache Spark中的一个核心模块，专门用于处理<strong>结构化数据</strong>。<br>它有两个主要特点：</p>
<ol>
<li><strong>统一入口</strong>：无论您的数据存储在何处（例如，JSON文件、CSV文件、数据库表，或者Hive表），您都可以使用标准的SQL查询语句来对它们进行查询和分析。</li>
<li><strong>高性能</strong>：Spark SQL底层利用了Spark核心引擎的先进技术（如内存计算和强大的Catalyst优化器），使得SQL查询的执行速度非常快，比传统的Hive on MapReduce要快得多。</li>
</ol>
<p>在使用SQL前，要先准备环境和数据，示例代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个SparkSession实例</span></span><br><span class="line">spark = SparkSession.builder \</span><br><span class="line">    .appName(<span class="string">&quot;SparkSQLExample&quot;</span>) \</span><br><span class="line">    .getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为了方便演示，我们不从文件读取，而是直接在内存中创建数据</span></span><br><span class="line"><span class="comment"># 这等同于我们有一个名为 &quot;scores.csv&quot; 的文件，内容如下：</span></span><br><span class="line"><span class="comment"># name,subject,score</span></span><br><span class="line"><span class="comment"># Alice,Math,95</span></span><br><span class="line"><span class="comment"># Bob,Math,88</span></span><br><span class="line"><span class="comment"># Alice,English,89</span></span><br><span class="line"><span class="comment"># Charlie,Math,92</span></span><br><span class="line"><span class="comment"># Bob,English,94</span></span><br><span class="line">data = [(<span class="string">&quot;Alice&quot;</span>, <span class="string">&quot;Math&quot;</span>, <span class="number">95</span>),</span><br><span class="line">        (<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;Math&quot;</span>, <span class="number">88</span>),</span><br><span class="line">        (<span class="string">&quot;Alice&quot;</span>, <span class="string">&quot;English&quot;</span>, <span class="number">89</span>),</span><br><span class="line">        (<span class="string">&quot;Charlie&quot;</span>, <span class="string">&quot;Math&quot;</span>, <span class="number">92</span>),</span><br><span class="line">        (<span class="string">&quot;Bob&quot;</span>, <span class="string">&quot;English&quot;</span>, <span class="number">94</span>)]</span><br><span class="line">columns = [<span class="string">&quot;name&quot;</span>, <span class="string">&quot;subject&quot;</span>, <span class="string">&quot;score&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据转换成Spark的结构化数据格式：DataFrame</span></span><br><span class="line">df = spark.createDataFrame(data, columns)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;成功创建DataFrame，数据如下：&quot;</span>)</span><br><span class="line">df.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>现在虽然我们有了一个df，但是想要用上Spark SQL,我们还得为他注册一个表名，示例代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将DataFrame注册为一个临时的SQL视图（表）</span></span><br><span class="line"></span><br><span class="line">df.createOrReplaceTempView(<span class="string">&quot;scores_table&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;DataFrame已成功注册为名为 &#x27;scores_table&#x27; 的临时表。&quot;</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p><em><strong>createOrReplaceTempView</strong></em>: 这是连接DataFrame世界和SQL世界的<strong>桥梁</strong>。它创建了一个只在当前 <code>SparkSession</code> 中有效的临时表。这个表是“虚拟”的，它并不真正复制数据，只是给 <code>df</code> 起了一个SQL可以识别的别名。会话结束后，这个临时表会自动消失。</p>
</blockquote>
<p>现在我们可以用SQL查询数据了，示例SQL：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用 spark.sql() 来执行标准的SQL查询</span></span><br><span class="line">high_scores_df = spark.sql(<span class="string">&quot;SELECT name, score FROM scores_table WHERE score &gt; 90&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;查询所有分数高于90分的学生：&quot;</span>)</span><br><span class="line">high_scores_df.show()</span><br></pre></td></tr></table></figure>

<blockquote>
<p><em><strong>spark.sql()</strong></em>: 这是Spark SQL的执行入口。您可以在括号里的字符串中编写任何标准的SQL语句。它的返回值是<strong>另一个DataFrame</strong>。这是Spark非常优雅的一点：无论您使用DataFrame API还是SQL，输入和输出的类型都是统一的。</p>
</blockquote>
<h4 id="Hive-和-Spark-SQL的异同"><a href="#Hive-和-Spark-SQL的异同" class="headerlink" title="Hive 和 Spark SQL的异同"></a>Hive 和 Spark SQL的异同</h4><ol>
<li>Hive是传统的MapReduce，速度会比Spark慢；</li>
<li>Hive是永久的，Spark加载到内存里面，数据等，会消失；因此Hive适合作为数据仓库，Spark适合快速探索数据</li>
<li>Spark SQL可以创建<code>TempView</code>（临时视图），非常灵活；Hive的表是永久记录在<strong>Metastore</strong>（元数据中心）中的，更持久、更严格。</li>
</ol>
<p>**注：Spark SQL非常强大，它也可以连接到Hive的Metastore，把Hive的永久表当作自己的数据源来查询。这时候，Spark SQL就扮演了一个“更快的叉车”角色，去Hive的“仓储超市”里拉货，这被称为 <strong>“Spark on Hive”</strong>。</p>
<h3 id="对比表格"><a href="#对比表格" class="headerlink" title="对比表格"></a><strong>对比表格</strong></h3><table>
<thead>
<tr>
<th align="left">特性</th>
<th align="left">Hive (传统中央厨房)</th>
<th align="left">Spark SQL (现代智能厨房)</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>执行引擎</strong></td>
<td align="left"><strong>MapReduce</strong> (基于磁盘，分步，慢)</td>
<td align="left"><strong>Spark Core</strong> (基于内存, 流水线, 快)</td>
</tr>
<tr>
<td align="left"><strong>核心数据模型</strong></td>
<td align="left">文件 (HDFS) + 元数据 (Metastore)</td>
<td align="left"><strong>DataFrame&#x2F;Dataset</strong> (分布式内存对象)</td>
</tr>
<tr>
<td align="left"><strong>查询优化</strong></td>
<td align="left">基础的优化</td>
<td align="left"><strong>Catalyst优化器</strong> (非常智能，会自动改写和优化查询计划)</td>
</tr>
<tr>
<td align="left"><strong>数据源</strong></td>
<td align="left">主要依赖HDFS和Hive表</td>
<td align="left"><strong>多样化</strong> (HDFS, S3, JSON, CSV, JDBC, Hive表等)</td>
</tr>
<tr>
<td align="left"><strong>交互性</strong></td>
<td align="left"><strong>低</strong> (查询通常需要几分钟到几小时)</td>
<td align="left"><strong>高</strong> (查询通常在几秒到几分钟内完成)</td>
</tr>
<tr>
<td align="left"><strong>应用场景</strong></td>
<td align="left">数据仓库、大规模ETL、批处理报表</td>
<td align="left">交互式分析、数据科学、机器学习、实时流处理（Structured Streaming）</td>
</tr>
<tr>
<td align="left"><strong>与对方关系</strong></td>
<td align="left">可以被Spark SQL作为数据源使用（Spark on Hive）</td>
<td align="left">可以兼容并加速对Hive表的查询</td>
</tr>
</tbody></table>
<h3 id="场景一：操作-HDFS-上的文件-以CSV文件为例"><a href="#场景一：操作-HDFS-上的文件-以CSV文件为例" class="headerlink" title="场景一：操作 HDFS 上的文件 (以CSV文件为例)"></a><strong>场景一：操作 HDFS 上的文件 (以CSV文件为例)</strong></h3><p>假如，你的数据是以CSV文件的形式存放在HDFS（Hadoop分布式文件系统）中的。</p>
<h4 id="第1步：读取-Read-HDFS上的CSV文件"><a href="#第1步：读取-Read-HDFS上的CSV文件" class="headerlink" title="第1步：读取 (Read) HDFS上的CSV文件"></a><strong>第1步：读取 (Read) HDFS上的CSV文件</strong></h4><p>假设HDFS上有一个路径为 <code>/user/data/employees.csv</code> 的文件。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这是创建SparkSession的通用代码，我们先要有这个“总指挥官”</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">&quot;HDFSExample&quot;</span>).getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 读取操作 ---</span></span><br><span class="line"><span class="comment"># 命令spark去读取一个csv文件</span></span><br><span class="line"><span class="comment"># .option(&quot;header&quot;, &quot;true&quot;) 表示CSV文件的第一行是列名</span></span><br><span class="line"><span class="comment"># .option(&quot;inferSchema&quot;, &quot;true&quot;) 表示让Spark自动推断列的数据类型（如整数、字符串等）</span></span><br><span class="line"><span class="comment"># .load() 指定文件的HDFS路径</span></span><br><span class="line">df = spark.read.<span class="built_in">format</span>(<span class="string">&quot;csv&quot;</span>) \</span><br><span class="line">              .option(<span class="string">&quot;header&quot;</span>, <span class="string">&quot;true&quot;</span>) \</span><br><span class="line">              .option(<span class="string">&quot;inferSchema&quot;</span>, <span class="string">&quot;true&quot;</span>) \</span><br><span class="line">              .load(<span class="string">&quot;hdfs://&lt;your-namenode-host&gt;:8020/user/data/employees.csv&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为了确认数据已经读进来，我们可以看一下它的结构和前几行</span></span><br><span class="line">df.printSchema() <span class="comment"># 打印表结构</span></span><br><span class="line">df.show(<span class="number">5</span>)       <span class="comment"># 显示前5行数据</span></span><br></pre></td></tr></table></figure>

<p><strong>代码讲解</strong>：</p>
<ul>
<li><code>spark.read</code>：这是告诉Spark我们要开始“读取”数据了。</li>
<li><code>.format(&quot;csv&quot;)</code>：明确告诉Spark，我们要读的是CSV格式的文件。</li>
<li><code>.load(&quot;hdfs://...&quot;)</code>：给出文件的具体位置。这就好比告诉快递员要去哪个地址取包裹。</li>
<li>读取成功后，数据就被装载成一个名为 <code>df</code> 的DataFrame（可以理解为内存中的一张智能表格）。</li>
</ul>
<h4 id="第2步：处理-Process-数据"><a href="#第2步：处理-Process-数据" class="headerlink" title="第2步：处理 (Process) 数据"></a><strong>第2步：处理 (Process) 数据</strong></h4><p>现在数据已经在 <code>df</code> 这个DataFrame里了。我们可以用PySpark的函数对它进行各种处理。比如，我们想筛选出所有“IT部门”并且年龄大于30岁的员工。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># --- 处理操作 ---</span></span><br><span class="line"><span class="comment"># .filter() 就是筛选/过滤操作</span></span><br><span class="line">processed_df = df.<span class="built_in">filter</span>((df[<span class="string">&#x27;department&#x27;</span>] == <span class="string">&#x27;IT&#x27;</span>) &amp; (df[<span class="string">&#x27;age&#x27;</span>] &gt; <span class="number">30</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 看一下处理结果</span></span><br><span class="line">processed_df.show()</span><br></pre></td></tr></table></figure>
<p><strong>代码讲解</strong>：</p>
<ul>
<li><code>df.filter(...)</code>：对 <code>df</code> 这张表格执行筛选操作，只保留满足括号内条件的行。简单直观！</li>
</ul>
<h4 id="第3步：写回-Write-到HDFS"><a href="#第3步：写回-Write-到HDFS" class="headerlink" title="第3步：写回 (Write) 到HDFS"></a><strong>第3步：写回 (Write) 到HDFS</strong></h4><p>处理完成后，我们想把结果 <code>processed_df</code> 保存回HDFS的一个新位置，比如存成Parquet格式（一种性能更好的列式存储格式）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># --- 写回操作 ---</span></span><br><span class="line"><span class="comment"># .write 是告诉Spark我们要“写入”数据了</span></span><br><span class="line"><span class="comment"># .mode(&quot;overwrite&quot;) 表示如果目标路径已存在，就覆盖它。其他选项有 &quot;append&quot;, &quot;ignore&quot;, &quot;error&quot;</span></span><br><span class="line"><span class="comment"># .save() 指定要保存到的HDFS路径</span></span><br><span class="line">processed_df.write.<span class="built_in">format</span>(<span class="string">&quot;parquet&quot;</span>) \</span><br><span class="line">                  .mode(<span class="string">&quot;overwrite&quot;</span>) \</span><br><span class="line">                  .save(<span class="string">&quot;hdfs://&lt;your-namenode-host&gt;:8020/user/processed_data/it_employees&quot;</span>)</span><br></pre></td></tr></table></figure>
<p><strong>代码讲解</strong>：</p>
<ul>
<li><code>processed_df.write</code>：告诉Spark我们要把 <code>processed_df</code> 这张表的内容写出去。</li>
<li><code>.format(&quot;parquet&quot;)</code>：指定用Parquet格式保存，这在大数据领域非常流行。</li>
<li><code>.mode(&quot;overwrite&quot;)</code>：这是一个非常重要的写入模式设置，<code>overwrite</code>会覆盖已有数据，非常适合重复运行的脚本。</li>
<li><code>.save(&quot;hdfs://...&quot;)</code>：给出保存包裹的目标地址。</li>
</ul>
<hr>
<h3 id="场景二：操作-Hive-仓库中的表"><a href="#场景二：操作-Hive-仓库中的表" class="headerlink" title="场景二：操作 Hive 仓库中的表"></a><strong>场景二：操作 Hive 仓库中的表</strong></h3><p>现在，假设你的数据源是Hive里的一张表，而不是HDFS上的一个独立文件。</p>
<h4 id="第1步：读取-Read-Hive表"><a href="#第1步：读取-Read-Hive表" class="headerlink" title="第1步：读取 (Read) Hive表"></a><strong>第1步：读取 (Read) Hive表</strong></h4><p>在连接Hive之前，创建<code>SparkSession</code>时需要加一点“佐料”：<code>.enableHiveSupport()</code>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建支持Hive的SparkSession</span></span><br><span class="line"><span class="comment"># 这里的 .enableHiveSupport() 是关键！</span></span><br><span class="line">spark_hive = SparkSession.builder.appName(<span class="string">&quot;HiveExample&quot;</span>) \</span><br><span class="line">                           .enableHiveSupport() \</span><br><span class="line">                           .getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 读取操作 ---</span></span><br><span class="line"><span class="comment"># 假设Hive里有一个数据库叫 `my_db`，里面有张表叫 `employees`</span></span><br><span class="line"><span class="comment"># 使用 .table() 方法直接读取，非常方便</span></span><br><span class="line">hive_df = spark_hive.read.table(<span class="string">&quot;my_db.employees&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 或者，你也可以用SparkSQL来做同样的事情，效果完全一样</span></span><br><span class="line"><span class="comment"># 这就是SparkSQL的威力！</span></span><br><span class="line">hive_df_sql = spark_hive.sql(<span class="string">&quot;SELECT * FROM my_db.employees&quot;</span>)</span><br><span class="line"></span><br><span class="line">hive_df.show(<span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<p><strong>代码讲解</strong>：</p>
<ul>
<li><code>.enableHiveSupport()</code>：这个命令让<code>SparkSession</code>能够连接到Hive的元数据仓库（Metastore），从而知道<code>my_db.employees</code>这张表在哪里、长什么样。</li>
<li><code>spark_hive.read.table(&quot;my_db.employees&quot;)</code>：这是读取Hive表最直接的方式。因为Spark已经通过元数据知道了表的一切信息，所以你只需要提供表名即可。</li>
</ul>
<h4 id="第2步：处理-Process-数据-1"><a href="#第2步：处理-Process-数据-1" class="headerlink" title="第2步：处理 (Process) 数据"></a><strong>第2步：处理 (Process) 数据</strong></h4><p>处理方式和操作HDFS文件读进来的DataFrame完全一样！比如，我们按部门计算平均工资。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># --- 处理操作 ---</span></span><br><span class="line"><span class="comment"># .groupBy() 是分组操作，.agg() 是聚合操作</span></span><br><span class="line"><span class="comment"># 这里计算每个部门(department)的平均工资(salary)</span></span><br><span class="line">salary_summary_df = hive_df.groupBy(<span class="string">&quot;department&quot;</span>) \</span><br><span class="line">                           .agg(&#123;<span class="string">&quot;salary&quot;</span>: <span class="string">&quot;avg&quot;</span>&#125;) \</span><br><span class="line">                           .withColumnRenamed(<span class="string">&quot;avg(salary)&quot;</span>, <span class="string">&quot;average_salary&quot;</span>) <span class="comment"># 给计算出的列改个好听的名字</span></span><br><span class="line"></span><br><span class="line">salary_summary_df.show()</span><br></pre></td></tr></table></figure>
<p><strong>代码讲解</strong>：</p>
<ul>
<li><code>groupBy(&quot;department&quot;)</code>：按“部门”列进行分组。</li>
<li><code>.agg({&quot;salary&quot;: &quot;avg&quot;})</code>：对每个分组后的数据，计算“salary”列的平均值（avg）。</li>
</ul>
<h4 id="第3步：写回-Write-到Hive"><a href="#第3步：写回-Write-到Hive" class="headerlink" title="第3步：写回 (Write) 到Hive"></a><strong>第3步：写回 (Write) 到Hive</strong></h4><p>我们可以将处理结果 <code>salary_summary_df</code> 保存为一张新的Hive表。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># --- 写回操作 ---</span></span><br><span class="line"><span class="comment"># .saveAsTable() 可以将DataFrame直接保存为一张Hive表</span></span><br><span class="line">salary_summary_df.write.mode(<span class="string">&quot;overwrite&quot;</span>).saveAsTable(<span class="string">&quot;my_db.salary_summary&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 你可以立即用SQL查询这张新表，验证写入是否成功</span></span><br><span class="line">spark_hive.sql(<span class="string">&quot;SELECT * FROM my_db.salary_summary&quot;</span>).show()</span><br></pre></td></tr></table></figure>
<p><strong>代码讲解</strong>：</p>
<ul>
<li><code>.saveAsTable(&quot;my_db.salary_summary&quot;)</code>：这是将DataFrame写入为Hive表的核心命令。Spark会自动处理底层的HDFS文件存储和Hive元数据更新，非常省心。</li>
</ul>
<hr>
<p><strong>总结一下</strong>：</p>
<ul>
<li><strong>对HDFS文件</strong>：<code>spark.read.format(...).load(&quot;路径&quot;)</code> -&gt; 处理 -&gt; <code>df.write.format(...).save(&quot;路径&quot;)</code></li>
<li><strong>对Hive表</strong>：<code>spark.read.table(&quot;库.表&quot;)</code> 或 <code>spark.sql(&quot;SELECT...&quot;)</code> -&gt; 处理 -&gt; <code>df.write.saveAsTable(&quot;库.表&quot;)</code></li>
<li><strong>关键区别</strong>：操作Hive需要<code>enableHiveSupport()</code>，并且读写时用的是<strong>表名</strong>，而操作HDFS用的是<strong>文件路径</strong>。</li>
</ul>
<h2 id="核心概念"><a href="#核心概念" class="headerlink" title="核心概念"></a>核心概念</h2><h3 id="Driver"><a href="#Driver" class="headerlink" title="Driver"></a>Driver</h3><p>接到需求后，进行规划，制定计划。一个计划由多个阶段（Stage）组成，分阶段的依据是 是否需要进行 Shuffle</p>
<h3 id="Stage"><a href="#Stage" class="headerlink" title="Stage"></a>Stage</h3><p>一个Stage会有很多可以同时进行的、性质一样的小任务（Task）</p>
<h3 id="Shuffle"><a href="#Shuffle" class="headerlink" title="Shuffle"></a>Shuffle</h3><p>发生在两个有依赖关系的Stage之间</p>
<h3 id="Executor"><a href="#Executor" class="headerlink" title="Executor"></a>Executor</h3><p>真正执行任务的进程，会去Driver那里领取任务</p>
<h3 id="task"><a href="#task" class="headerlink" title="task"></a>task</h3><p>最小执行单元</p>
</article>
<div class="article-footer">
    <section id="license">
      <div class="header"><span>许可协议</span></div>
      <div class="body"><p>本文由Chen Dongcheng (Cdc)编写，采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">署名-非商业性使用-相同方式共享 4.0 国际</a> 许可协议，转载请注明出处。</p>
</div>
    </section>
    

</div>

<div class="related-wrap" id="read-next"><section class="body"><div class="item" id="prev"><div class="note">较新文章</div><a href="/2026/01/23/vscode-copilot%E6%8E%A5%E7%AC%AC%E4%B8%89%E6%96%B9api/">vscode_copilot接第三方api</a></div><div class="item" id="next"><div class="note">较早文章</div><a href="/2026/01/22/vscode_and_python/">VS Code 遇到的 No module named 问题</a></div></section></div>




  <div class="related-wrap md-text" id="comments">
    <section class='header cmt-title cap theme'>
      <p>快来参与讨论吧~</p>

    </section>
    <section class='body cmt-body beaudar'>
      

<svg class="loading" style="vertical-align:middle;fill:currentColor;overflow:hidden;" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="2709"><path d="M832 512c0-176-144-320-320-320V128c211.2 0 384 172.8 384 384h-64zM192 512c0 176 144 320 320 320v64C300.8 896 128 723.2 128 512h64z" p-id="2710"></path></svg>

<div id="beaudar" repo="PRose2077/blog-comment" issue-term="pathname" theme="preferred-color-scheme" input-position="top" comment-order="desc" loading="false" branch="main"></div>

    </section>
  </div>



<footer class="page-footer footnote"><hr><div class="text"><p>本站由 <a href="/">我们</a> 使用 <a target="_blank" rel="noopener" href="https://github.com/xaoxuu/hexo-theme-stellar/tree/1.33.1">Stellar 1.33.1</a> 主题创建。<br>本博客所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议，转载请注明出处。</p>
</div></footer>
<div class="main-mask" onclick="sidebar.dismiss()"></div></div><aside class="l_right">
<div class="widgets">



<widget class="widget-wrapper toc" id="data-toc" collapse="false"><div class="widget-header dis-select"><span class="name">本文目录</span><a class="cap-action" onclick="sidebar.toggleTOC()" ><svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6h11m-11 6h11m-11 6h11M4 6h1v4m-1 0h2m0 8H4c0-1 2-2 2-3s-1-1.5-2-1"/></svg></a></div><div class="widget-body"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark%E7%9A%84%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5"><span class="toc-text">Spark的核心概念</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-RDD%E2%80%93-Spark%E7%9A%84%E6%A0%B8%E5%BF%83%E5%9F%BA%E7%9F%B3"><span class="toc-text">1.RDD–&gt;Spark的核心基石</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-Spark%E7%9A%84%E4%B8%BB%E6%B5%81API-dataFrame"><span class="toc-text">2.Spark的主流API-dataFrame</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-DataSet-DataFrame%E7%9A%84%E6%89%A9%E5%B1%95"><span class="toc-text">3.DataSet -&gt; DataFrame的扩展</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3Spark%E7%9B%B8%E6%AF%94%E4%BA%8EMapReduce%E7%9A%84%E4%BC%98%E5%8A%BF%EF%BC%9F"><span class="toc-text">如何理解Spark相比于MapReduce的优势？</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#MapReduce"><span class="toc-text">MapReduce</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Spark"><span class="toc-text">Spark</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8-PySpark-DataFrame-API-%E4%BB%8E-HDFS-%E8%AF%BB%E5%8F%96%E6%96%87%E4%BB%B6%E5%B9%B6%E8%BF%9B%E8%A1%8C%E6%93%8D%E4%BD%9C"><span class="toc-text">如何使用 PySpark DataFrame API 从 HDFS 读取文件并进行操作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SparkSQL"><span class="toc-text">SparkSQL</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Hive-%E5%92%8C-Spark-SQL%E7%9A%84%E5%BC%82%E5%90%8C"><span class="toc-text">Hive 和 Spark SQL的异同</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AF%B9%E6%AF%94%E8%A1%A8%E6%A0%BC"><span class="toc-text">对比表格</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9C%BA%E6%99%AF%E4%B8%80%EF%BC%9A%E6%93%8D%E4%BD%9C-HDFS-%E4%B8%8A%E7%9A%84%E6%96%87%E4%BB%B6-%E4%BB%A5CSV%E6%96%87%E4%BB%B6%E4%B8%BA%E4%BE%8B"><span class="toc-text">场景一：操作 HDFS 上的文件 (以CSV文件为例)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AC%AC1%E6%AD%A5%EF%BC%9A%E8%AF%BB%E5%8F%96-Read-HDFS%E4%B8%8A%E7%9A%84CSV%E6%96%87%E4%BB%B6"><span class="toc-text">第1步：读取 (Read) HDFS上的CSV文件</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AC%AC2%E6%AD%A5%EF%BC%9A%E5%A4%84%E7%90%86-Process-%E6%95%B0%E6%8D%AE"><span class="toc-text">第2步：处理 (Process) 数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AC%AC3%E6%AD%A5%EF%BC%9A%E5%86%99%E5%9B%9E-Write-%E5%88%B0HDFS"><span class="toc-text">第3步：写回 (Write) 到HDFS</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9C%BA%E6%99%AF%E4%BA%8C%EF%BC%9A%E6%93%8D%E4%BD%9C-Hive-%E4%BB%93%E5%BA%93%E4%B8%AD%E7%9A%84%E8%A1%A8"><span class="toc-text">场景二：操作 Hive 仓库中的表</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AC%AC1%E6%AD%A5%EF%BC%9A%E8%AF%BB%E5%8F%96-Read-Hive%E8%A1%A8"><span class="toc-text">第1步：读取 (Read) Hive表</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AC%AC2%E6%AD%A5%EF%BC%9A%E5%A4%84%E7%90%86-Process-%E6%95%B0%E6%8D%AE-1"><span class="toc-text">第2步：处理 (Process) 数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%AC%AC3%E6%AD%A5%EF%BC%9A%E5%86%99%E5%9B%9E-Write-%E5%88%B0Hive"><span class="toc-text">第3步：写回 (Write) 到Hive</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5"><span class="toc-text">核心概念</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Driver"><span class="toc-text">Driver</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Stage"><span class="toc-text">Stage</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Shuffle"><span class="toc-text">Shuffle</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Executor"><span class="toc-text">Executor</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#task"><span class="toc-text">task</span></a></li></ol></li></ol></div><div class="widget-footer"><a class="top" onclick="util.scrollTop()"><svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><!-- Icon from Solar by 480 Design - https://creativecommons.org/licenses/by/4.0/ --><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-width="1.5"><path stroke-linejoin="round" d="m9 15.5l3-3l3 3m-6-4l3-3l3 3"/><path d="M7 3.338A9.95 9.95 0 0 1 12 2c5.523 0 10 4.477 10 10s-4.477 10-10 10S2 17.523 2 12c0-1.821.487-3.53 1.338-5"/></g></svg><span>回到顶部</span></a><a class="buttom" onclick="util.scrollComment()"><svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><!-- Icon from Solar by 480 Design - https://creativecommons.org/licenses/by/4.0/ --><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-width="1.5" d="M8 10.5h8M8 14h5.5M17 3.338A9.95 9.95 0 0 0 12 2C6.477 2 2 6.477 2 12c0 1.6.376 3.112 1.043 4.453c.178.356.237.763.134 1.148l-.595 2.226a1.3 1.3 0 0 0 1.591 1.592l2.226-.596a1.63 1.63 0 0 1 1.149.133A9.96 9.96 0 0 0 12 22c5.523 0 10-4.477 10-10c0-1.821-.487-3.53-1.338-5"/></svg><span>参与讨论</span></a></div></widget>
</div></aside><div class='float-panel'>
  <button type='button' style='display:none' class='laptop-only rightbar-toggle mobile' onclick='sidebar.rightbar()'>
    <svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6h11m-11 6h11m-11 6h11M4 6h1v4m-1 0h2m0 8H4c0-1 2-2 2-3s-1-1.5-2-1"/></svg>
  </button>
  <button type='button' style='display:none' class='mobile-only leftbar-toggle mobile' onclick='sidebar.leftbar()'>
    <svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><g fill="none" stroke="currentColor" stroke-width="1.5"><path d="M2 11c0-3.771 0-5.657 1.172-6.828C4.343 3 6.229 3 10 3h4c3.771 0 5.657 0 6.828 1.172C22 5.343 22 7.229 22 11v2c0 3.771 0 5.657-1.172 6.828C19.657 21 17.771 21 14 21h-4c-3.771 0-5.657 0-6.828-1.172C2 18.657 2 16.771 2 13z"/><path id="sep" stroke-linecap="round" d="M5.5 10h6m-5 4h4m4.5 7V3"/></g></svg>
  </button>
</div>
</div><div class="scripts">


<script type="text/javascript">
  window.canonical = {"originalHost":null,"officialHosts":["localhost"],"encoded":""};
  const ctx = {
    date_suffix: {
      just: `刚刚`,
      min: `分钟前`,
      hour: `小时前`,
      day: `天前`,
    },
    root : `/`,
    tag_plugins: {
      chat: Object.assign({"api":"https://siteinfo.listentothewind.cn/api/v1"}),
    }
  };

  // required plugins (only load if needs)
  if (`local_search`) {
    ctx.search = {};
    ctx.search.service = `local_search`;
    if (ctx.search.service == 'local_search') {
      let service_obj = Object.assign({}, `{"field":"all","path":"/search.json","content":true,"skip_search":null,"sort":"-date"}`);
      ctx.search[ctx.search.service] = service_obj;
    }
  }
  const def = {
    avatar: `https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.12/avatar/round/3442075.svg`,
    cover: `https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.12/cover/76b86c0226ffd.svg`,
    loading: `https://api.iconify.design/eos-icons:three-dots-loading.svg?color=%231cd0fd`,
  };
  const deps = {
    jquery: `https://gcore.jsdelivr.net/npm/jquery@3.7/dist/jquery.min.js`,
    marked: `https://gcore.jsdelivr.net/npm/marked@13.0/lib/marked.umd.min.js`,
    lazyload: `/%5Bobject%20Object%5D`
  }
  

</script>

<script type="text/javascript">
  
  function RunItem() {
    this.list = []; // 存放回调函数
    this.start = () => {
      for (var i = 0; i < this.list.length; i++) {
        this.list[i].run();
      }
    };
    this.push = (fn, name, setRequestAnimationFrame = true) => {
      let myfn = fn
      if (setRequestAnimationFrame) {
        myfn = () => {
          utils.requestAnimationFrame(fn)
        }
      }
      var f = new Item(myfn, name);
      this.list.push(f);
    };
    this.remove = (name) => {
      for (let index = 0; index < this.list.length; index++) {
        const e = this.list[index];
        if (e.name == name) {
          this.list.splice(index, 1);
        }
      }
    }
    // 构造一个可以run的对象
    function Item(fn, name) {
      // 函数名称
      this.name = name || fn.name;
      // run方法
      this.run = () => {
        try {
          fn()
        } catch (error) {
          console.log(error);
        }
      };
    }
  }

  const utils = {
    // 懒加载 css https://github.com/filamentgroup/loadCSS
    css: (href, before, media, attributes) => {
      var doc = window.document;
      var ss = doc.createElement("link");
      var ref;
      if (before) {
        ref = before;
      } else {
        var refs = (doc.body || doc.getElementsByTagName("head")[0]).childNodes;
        ref = refs[refs.length - 1];
      }
      var sheets = doc.styleSheets;
      if (attributes) {
        for (var attributeName in attributes) {
          if (attributes.hasOwnProperty(attributeName)) {
            ss.setAttribute(attributeName, attributes[attributeName]);
          }
        }
      }
      ss.rel = "stylesheet";
      ss.href = href;
      ss.media = "only x";
      function ready(cb) {
        if (doc.body) {
          return cb();
        }
        setTimeout(function () {
          ready(cb);
        });
      }
      ready(function () {
        ref.parentNode.insertBefore(ss, before ? ref : ref.nextSibling);
      });
      var onloadcssdefined = function (cb) {
        var resolvedHref = ss.href;
        var i = sheets.length;
        while (i--) {
          if (sheets[i].href === resolvedHref) {
            return cb();
          }
        }
        setTimeout(function () {
          onloadcssdefined(cb);
        });
      };
      function loadCB() {
        if (ss.addEventListener) {
          ss.removeEventListener("load", loadCB);
        }
        ss.media = media || "all";
      }
      if (ss.addEventListener) {
        ss.addEventListener("load", loadCB);
      }
      ss.onloadcssdefined = onloadcssdefined;
      onloadcssdefined(loadCB);
      return ss;
    },

    js: (src, opt) => new Promise((resolve, reject) => {
      var script = document.createElement('script');
      if (src.startsWith('/')) {
        src = ctx.root + src.substring(1);
      }
      script.src = src;
      if (opt) {
        for (let key of Object.keys(opt)) {
          script[key] = opt[key]
        }
      } else {
        // 默认异步，如果需要同步，第二个参数传入 {} 即可
        script.async = true
      }
      script.onerror = reject
      script.onload = script.onreadystatechange = function () {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    }),

    jq: (fn) => {
      if (typeof jQuery === 'undefined') {
        utils.js(deps.jquery).then(fn)
      } else {
        fn()
      }
    },

    onLoading: (el) => {
      if (el) {
        $(el).append('<div class="loading-wrap"><svg xmlns="http://www.w3.org/2000/svg" width="2em" height="2em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-width="2"><path stroke-dasharray="60" stroke-dashoffset="60" stroke-opacity=".3" d="M12 3C16.9706 3 21 7.02944 21 12C21 16.9706 16.9706 21 12 21C7.02944 21 3 16.9706 3 12C3 7.02944 7.02944 3 12 3Z"><animate fill="freeze" attributeName="stroke-dashoffset" dur="1.3s" values="60;0"/></path><path stroke-dasharray="15" stroke-dashoffset="15" d="M12 3C16.9706 3 21 7.02944 21 12"><animate fill="freeze" attributeName="stroke-dashoffset" dur="0.3s" values="15;0"/><animateTransform attributeName="transform" dur="1.5s" repeatCount="indefinite" type="rotate" values="0 12 12;360 12 12"/></path></g></svg></div>');
      }
    },
    onLoadSuccess: (el) => {
      if (el) {
        $(el).find('.loading-wrap').remove();
      }
    },
    onLoadFailure: (el) => {
      if (el) {
        $(el).find('.loading-wrap svg').remove();
        $(el).find('.loading-wrap').append('<svg xmlns="http://www.w3.org/2000/svg" width="2em" height="2em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"><path stroke-dasharray="60" stroke-dashoffset="60" d="M12 3L21 20H3L12 3Z"><animate fill="freeze" attributeName="stroke-dashoffset" dur="0.5s" values="60;0"/></path><path stroke-dasharray="6" stroke-dashoffset="6" d="M12 10V14"><animate fill="freeze" attributeName="stroke-dashoffset" begin="0.6s" dur="0.2s" values="6;0"/></path></g><circle cx="12" cy="17" r="1" fill="currentColor" fill-opacity="0"><animate fill="freeze" attributeName="fill-opacity" begin="0.8s" dur="0.4s" values="0;1"/></circle></svg>');
        $(el).find('.loading-wrap').addClass('error');
      }
    },
    request: (el, url, callback, onFailure) => {
      const maxRetry = 3;
      let retryCount = 0;

      return new Promise((resolve, reject) => {
        const load = () => {
          utils.onLoading?.(el);

          let timedOut = false;
          const timeout = setTimeout(() => {
            timedOut = true;
            console.warn('[request] 超时:', url);

            if (++retryCount >= maxRetry) {
              utils.onLoadFailure?.(el);
              onFailure?.();
              reject('请求超时');
            } else {
              setTimeout(load, 1000);
            }
          }, 5000);

          fetch(url).then(resp => {
            if (timedOut) return;
            clearTimeout(timeout);

            if (!resp.ok) throw new Error('响应失败');
            return resp;
          }).then(data => {
            if (timedOut) return;
            utils.onLoadSuccess?.(el);
            callback(data);
            resolve(data);
          }).catch(err => {
            clearTimeout(timeout);
            console.warn('[request] 错误:', err);

            if (++retryCount >= maxRetry) {
              utils.onLoadFailure?.(el);
              onFailure?.();
              reject(err);
            } else {
              setTimeout(load, 1000);
            }
          });
        };

        load();
      });
    },
    requestWithoutLoading: (url, options = {}, maxRetry = 2, timeout = 5000) => {
      return new Promise((resolve, reject) => {
        let retryCount = 0;

        const tryRequest = () => {
          let timedOut = false;
          const timer = setTimeout(() => {
            timedOut = true;
            if (++retryCount > maxRetry) reject('timeout');
            else tryRequest();
          }, timeout);

          fetch(url, options)
            .then(resp => {
              clearTimeout(timer);
              if (!resp.ok) throw new Error('bad response');
              resolve(resp);
            })
            .catch(err => {
              clearTimeout(timer);
              if (++retryCount > maxRetry) reject(err);
              else setTimeout(tryRequest, 500);
            });
        };

        tryRequest();
      });
    },
    /********************** requestAnimationFrame ********************************/
    // 1、requestAnimationFrame 会把每一帧中的所有 DOM 操作集中起来，在一次重绘或回流中就完成，并且重绘或回流的时间间隔紧紧跟随浏览器的刷新频率，一般来说，这个频率为每秒60帧。
    // 2、在隐藏或不可见的元素中，requestAnimationFrame 将不会进行重绘或回流，这当然就意味着更少的的 cpu，gpu 和内存使用量。
    requestAnimationFrame: (fn) => {
      if (!window.requestAnimationFrame) {
        window.requestAnimationFrame = window.requestAnimationFrame || window.mozRequestAnimationFrame || window.webkitRequestAnimationFrame;
      }
      window.requestAnimationFrame(fn)
    },
    dark: {},
  };

  // utils.dark.mode 当前模式 dark or light
  // utils.dark.toggle() 暗黑模式触发器
  // utils.dark.push(callBack[,"callBackName"]) 传入触发器回调函数
  utils.dark.method = {
    toggle: new RunItem(),
  };
  utils.dark = Object.assign(utils.dark, {
    push: utils.dark.method.toggle.push,
  });
</script>
<script>
  const sidebar = {
    leftbar: () => {
      if (l_body) {
        l_body.toggleAttribute('leftbar');
        l_body.removeAttribute('rightbar');
      }
    },
    rightbar: () => {
      if (l_body) {
        l_body.toggleAttribute('rightbar');
        l_body.removeAttribute('leftbar');
      }
    },
    dismiss: () => {
      if (l_body) {
        l_body.removeAttribute('leftbar');
        l_body.removeAttribute('rightbar');
      }
    },
    toggleTOC: () => {
      document.querySelector('#data-toc').classList.toggle('collapse');
    }
  }
</script>
<script type="text/javascript">
  (() => {
    const tagSwitchers = document.querySelectorAll('.tag-subtree.parent-tag > a > .tag-switcher-wrapper')
    for (const tagSwitcher of tagSwitchers) {
      tagSwitcher.addEventListener('click', (e) => {
        const parent = e.target.closest('.tag-subtree.parent-tag')
        parent.classList.toggle('expanded')
        e.preventDefault()
      })
    }

    // Get active tag from query string, then activate it.
    const urlParams = new URLSearchParams(window.location.search)
    const activeTag = urlParams.get('tag')
    if (activeTag) {
      let tag = document.querySelector(`.tag-subtree[data-tag="${activeTag}"]`)
      if (tag) {
        tag.querySelector('a').classList.add('active')
        
        while (tag) {
          tag.classList.add('expanded')
          tag = tag.parentElement.closest('.tag-subtree.parent-tag')
        }
      }
    }
  })()
</script>

<script async src="https://gcore.jsdelivr.net/npm/vanilla-lazyload@19.1/dist/lazyload.min.js"></script>
<script>
  // https://www.npmjs.com/package/vanilla-lazyload
  // Set the options globally
  // to make LazyLoad self-initialize
  window.lazyLoadOptions = {
    elements_selector: ".lazy",
    callback_loaded: (el) => {
      el.classList.add('loaded');
      const wrapper = el.closest('.lazy-box');
      const icon = wrapper?.querySelector('.lazy-icon');
      if (icon) icon.remove();
    }
  };
  // Listen to the initialization event
  // and get the instance of LazyLoad
  window.addEventListener(
    "LazyLoad::Initialized",
    function (event) {
      window.lazyLoadInstance = event.detail.instance;
    },
    false
  );
  document.addEventListener('DOMContentLoaded', function () {
    window.lazyLoadInstance?.update();
  });

  window.wrapLazyloadImages = (container) => {
    if (typeof container === 'string') {
      container = document.querySelector(container);
    }
    if (!container) return;
    
    const images = container.querySelectorAll('img');
    images.forEach((img) => {
      if (img.classList.contains('lazy')) return;

      const src = img.getAttribute('src');
      if (!src) return;

      const wrapper = document.createElement('div');
      wrapper.className = 'lazy-box';

      const newImg = img.cloneNode();
      newImg.removeAttribute('src');
      newImg.setAttribute('data-src', src);
      newImg.classList.add('lazy');

      const icon = document.createElement('div');
      icon.className = 'lazy-icon';
      if (def.loading) {
        icon.style.backgroundImage = `url("${def.loading}")`;
      }

      wrapper.appendChild(newImg);
      wrapper.appendChild(icon);

      img.replaceWith(wrapper);
    });

    // 通知 LazyLoad 更新
    if (window.lazyLoadInstance?.update) {
      window.lazyLoadInstance.update();
    }
  }
  
</script>

<!-- required -->
<script src="/js/main.js?v=1.33.1" defer></script>

<script type="text/javascript">
  const applyTheme = (theme) => {
    if (theme === 'auto') {
      document.documentElement.removeAttribute('data-theme')
    } else {
      document.documentElement.setAttribute('data-theme', theme)
    }

    // applyThemeToGiscus(theme)
  }

  // FIXME: 这会导致无法使用 preferred_color_scheme 以外的主题
  const applyThemeToGiscus = (theme) => {
    // theme = theme === 'auto' ? 'preferred_color_scheme' : theme
    const cmt = document.getElementById('giscus')
    if (cmt) {
      // This works before giscus load.
      cmt.setAttribute('data-theme', theme)
    }

    const iframe = document.querySelector('#comments > section.giscus > iframe')
    if (iframe) {
      // This works after giscus loaded.
      const src = iframe.src
      const newSrc = src.replace(/theme=[\w]+/, `theme=${theme}`)
      iframe.src = newSrc
    }
  }

  const switchTheme = () => {
    // light -> dark -> auto -> light -> ...
    const currentTheme = document.documentElement.getAttribute('data-theme')
    let newTheme;
    switch (currentTheme) {
      case 'light':
        newTheme = 'dark'
        break
      case 'dark':
        newTheme = 'auto'
        break
      default:
        newTheme = 'light'
    }
    applyTheme(newTheme)
    window.localStorage.setItem('Stellar.theme', newTheme)
    utils.dark.mode = newTheme === 'auto' ? (window.matchMedia("(prefers-color-scheme: dark)").matches ? "dark" : "light") : newTheme;
    utils.dark.method.toggle.start();

    const messages = {
      light: `切换到浅色模式`,
      dark: `切换到深色模式`,
      auto: `切换到跟随系统配色`,
    }
    hud?.toast?.(messages[newTheme])
  }

  (() => {
    // Apply user's preferred theme, if any.
    const theme = window.localStorage.getItem('Stellar.theme')
    if (theme !== null) {
      applyTheme(theme)
    } else {
      utils.dark.mode = window.matchMedia("(prefers-color-scheme: dark)").matches ? "dark" : "light";
    }
    utils.dark.method.toggle.start();
  })()
</script>


<!-- optional -->

  <script type="module">
  const el = document.querySelector('#comments #beaudar');
  util.viewportLazyload(el, load_beaudar, false);

  function load_beaudar() {
    if (!el) return;
    try {
      el.innerHTML = '';
    } catch (error) {
      console.error(error);
    }
    const script = document.createElement('script');
    script.src = 'https://beaudar.lipk.org/client.js';
    script.async = true;
    for (const key of Object.keys(el.attributes)) {
      const attr = el.attributes[key];
      if (['class', 'id'].includes(attr.name) === false) {
        script.setAttribute(attr.name, attr.value);
      }
    }
    el.appendChild(script);
  }
</script>




<script defer>
  window.addEventListener('DOMContentLoaded', (event) => {
    ctx.services = Object.assign({}, JSON.parse(`{"mdrender":{"js":"/js/services/mdrender.js"},"siteinfo":{"js":"/js/services/siteinfo.js","api":null},"ghinfo":{"js":"/js/services/ghinfo.js"},"rating":{"js":"/js/services/rating.js","api":"https://star-vote.xaox.cc/api/rating"},"vote":{"js":"/js/services/vote.js","api":"https://star-vote.xaox.cc/api/vote"},"sites":{"js":"/js/services/sites.js"},"friends":{"js":"/js/services/friends.js"},"friends_and_posts":{"js":"/js/services/friends_and_posts.js"},"timeline":{"js":"/js/services/timeline.js"},"fcircle":{"js":"/js/services/fcircle.js"},"weibo":{"js":"/js/services/weibo.js"},"memos":{"js":"/js/services/memos.js"},"voice":{"js":"/js/plugins/voice.js"},"video":{"js":"/js/plugins/video.js"},"download-file":{"js":"/js/plugins/download-file.js"},"twikoo":{"js":"/js/services/twikoo_latest_comment.js"},"waline":{"js":"/js/services/waline_latest_comment.js"},"artalk":{"js":"/js/services/artalk_latest_comment.js"},"giscus":{"js":"/js/services/giscus_latest_comment.js"},"contributors":{"edit_this_page":{"_posts/":null,"wiki/stellar/":"https://github.com/xaoxuu/hexo-theme-stellar-docs/blob/main/"},"js":"/js/services/contributors.js"}}`));
    for (let id of Object.keys(ctx.services)) {
      const js = ctx.services[id].js;
      if (id == 'siteinfo') {
        ctx.cardlinks = document.querySelectorAll('a.link-card[cardlink]');
        if (ctx.cardlinks?.length > 0) {
          utils.js(js, { defer: true }).then(function () {
            setCardLink(ctx.cardlinks);
          });
        }
      } else if (id == 'voice') {
        ctx.voiceAudios = document.querySelectorAll('.voice>audio');
        if (ctx.voiceAudios?.length > 0) {
          utils.js(js, { defer: true }).then(function () {
            createVoiceDom(ctx.voiceAudios);
          });
        }
      } else if (id == 'video') {
        ctx.videos = document.querySelectorAll('.video>video');
        if (ctx.videos?.length > 0) {
          utils.js(js, { defer: true }).then(function () {
            videoEvents(ctx.videos);
          });
        }
      } else if (id == 'download-file') {
        ctx.files = document.querySelectorAll('.file');
        if (ctx.files?.length > 0) {
          utils.js(js, { defer: true }).then(function () {
            downloadFileEvent(ctx.files);
          });
        }
      } else {
        const els = document.getElementsByClassName(`ds-${id}`);
        if (els?.length > 0) {
          utils.jq(() => {
            if (id == 'timeline' || 'memos' || 'marked') {
              utils.js(deps.marked).then(function () {
                utils.js(js, { defer: true });
              });
            } else {
              utils.js(js, { defer: true });
            }
          });
        }
      }
    }

    // chat iphone time
    let phoneTimes = document.querySelectorAll('.chat .status-bar .time');

    if (phoneTimes.length > 0) {
      NowTime();
      var date = new Date();
      var sec = date.getSeconds();
      var firstAdjustInterval = setInterval(firstAdjustTime, 1000 * (60 - sec));
    }

    function firstAdjustTime() {
      NowTime();
      clearInterval(firstAdjustInterval);
      setInterval(NowTime, 1000 * 60);
    }

    function NowTime() {
      for (let i = 0; i < phoneTimes.length; ++i) {
        var timeSpan = phoneTimes[i];
        var date = new Date();
        var hour = date.getHours();
        var min = date.getMinutes();
        timeSpan.innerHTML = check(hour) + ":" + check(min);
      }
    };

    function check(val) {
      if (val < 10) {
        return ("0" + val);
      }
      return (val);
    }

    // chat quote
    const chat_quote_obverser = new IntersectionObserver((entries, observer) => {
      entries.filter((entry) => { return entry.isIntersecting }).sort((a, b) => a.intersectionRect.y !== b.intersectionRect.y ? a.intersectionRect.y - b.intersectionRect.y : a.intersectionRect.x - b.intersectionRect.x).forEach((entry, index) => {
          observer.unobserve(entry.target);
          setTimeout(() => {
            entry.target.classList.add('quote-blink');
            setTimeout(() => {
              entry.target.classList.remove('quote-blink');
            }, 1000);
          }, Math.max(100, 16) * (index + 1));
        });
    });

    var chatQuotes = document.querySelectorAll(".chat .talk .quote");
    chatQuotes.forEach((quote) => {
      quote.addEventListener('click', function () {
        var chatCellDom = document.getElementById("quote-" + quote.getAttribute("quotedCellTag"));
        if (chatCellDom) {
          var chatDiv = chatCellDom.parentElement;
          var mid = chatDiv.clientHeight / 2;
          var offsetTop = chatCellDom.offsetTop;
          if (offsetTop > mid - chatCellDom.clientHeight / 2) {
            chatDiv.scrollTo({
              top: chatCellDom.offsetTop - mid + chatCellDom.clientHeight / 2,
              behavior: "smooth"
            });
          } else {
            chatDiv.scrollTo({
              top: 0,
              behavior: "smooth"
            });
          }
          chat_quote_obverser.observe(chatCellDom);
        }
      });
    });
  });
</script>

<script>
  window.addEventListener('DOMContentLoaded', (event) => {
    ctx.search = {
      path: `/search.json`,
    }
    utils.js('/js/search/local-search.js', { defer: true });
  });
</script><script>
  window.FPConfig = {
    delay: 0,
    ignoreKeywords: [],
    maxRPS: 5,
    hoverDelay: 25
  };
</script>
<script defer src="https://gcore.jsdelivr.net/npm/flying-pages@2/flying-pages.min.js"></script><script>
  ctx.fancybox = {
    selector: `.timenode p>img`,
    css: `https://gcore.jsdelivr.net/npm/@fancyapps/ui@5.0/dist/fancybox/fancybox.css`,
    js: `https://gcore.jsdelivr.net/npm/@fancyapps/ui@5.0/dist/fancybox/fancybox.umd.js`
  };
  var selector = '[data-fancybox]:not(.error), .with-fancybox .atk-content img:not([atk-emoticon])';
  if (ctx.fancybox.selector) {
    selector += `, ${ctx.fancybox.selector}`
  }
  var needFancybox = document.querySelectorAll(selector).length !== 0;
  if (!needFancybox) {
    const memos = document.getElementsByClassName('ds-memos');
    if (memos != undefined && memos.length > 0) {
      needFancybox = true;
    }
    const fancybox = document.getElementsByClassName('with-fancybox');
    if (fancybox != undefined && fancybox.length > 0) {
      needFancybox = true;
    }
  }
  if (needFancybox) {
    utils.css(ctx.fancybox.css);
    utils.js(ctx.fancybox.js, { defer: true }).then(function () {
      Fancybox.bind(selector, {
        hideScrollbar: false,
        Thumbs: {
          autoStart: false,
        },
        caption: (fancybox, slide) => {
          return slide.triggerEl.alt || slide.triggerEl.dataset.caption || null
        }
      });
    })
  }
</script>
<script>
  window.addEventListener('DOMContentLoaded', (event) => {
    const swiper_api = document.getElementById('swiper-api');
    if (swiper_api != undefined) {
      utils.css(`https://unpkg.com/swiper@10.3/swiper-bundle.min.css`);
      utils.js(`https://unpkg.com/swiper@10.3/swiper-bundle.min.js`, { defer: true }).then(function () {
        const effect = swiper_api.getAttribute('effect') || '';
        var swiper = new Swiper('.swiper#swiper-api', {
          slidesPerView: 'auto',
          spaceBetween: 8,
          centeredSlides: true,
          effect: effect,
          rewind: true,
          pagination: {
            el: '.swiper-pagination',
            clickable: true,
          },
          navigation: {
            nextEl: '.swiper-button-next',
            prevEl: '.swiper-button-prev',
          },
        });
      })
    }
  });
</script>
<script>
  document.addEventListener('DOMContentLoaded', function () {
    window.codeElements = document.querySelectorAll('.code');
    if (window.codeElements.length > 0) {
      ctx.copycode = {
        default_text: `Copy`,
        success_text: `Copied`,
        toast: `复制成功`,
      };
      utils.js('/js/plugins/copycode.js');
    }
  });
</script>


<!-- inject -->

</div></body></html>
